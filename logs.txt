* 
* ==> Audit <==
* |------------|--------------------------------|----------|----------------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |         User         | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|----------------------|---------|---------------------|---------------------|
| start      |                                | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 29 Apr 24 16:59 +04 | 29 Apr 24 17:02 +04 |
| image      | load catalog-service.tar       | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 29 Apr 24 17:19 +04 | 29 Apr 24 17:20 +04 |
| image      | load catalog-service.tar       | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 29 Apr 24 17:22 +04 | 29 Apr 24 17:22 +04 |
| start      |                                | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 15 Mar 25 01:03 +04 |                     |
| start      |                                | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 15 Mar 25 01:03 +04 | 15 Mar 25 01:04 +04 |
| dashboard  | --port=63840                   | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 15 Mar 25 01:05 +04 |                     |
| start      |                                | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 15 Mar 25 01:07 +04 | 15 Mar 25 01:09 +04 |
| dashboard  | --port=63840                   | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 15 Mar 25 01:09 +04 |                     |
| image      | load                           | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 15 Mar 25 01:10 +04 |                     |
|            | catalog-service:0.0.1-SNAPSHOT |          |                      |         |                     |                     |
| stop       |                                | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 15 Mar 25 01:31 +04 | 15 Mar 25 01:31 +04 |
| start      | --cpus 2 --memory 3g           | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 09 Apr 25 15:40 +04 | 09 Apr 25 15:42 +04 |
| stop       |                                | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 09 Apr 25 16:17 +04 | 09 Apr 25 16:18 +04 |
| start      | --cpus 2 --memory 3g           | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 15:31 +04 | 18 Apr 25 15:32 +04 |
| dashboard  | --port=63840                   | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 15:35 +04 |                     |
| image      | load catalog-service:latest    | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 16:39 +04 |                     |
| image      | load catalog-service:latest    | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 16:44 +04 |                     |
| image      | load catalog-service:latest    | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 16:49 +04 |                     |
| docker-env | minikube docker-env            | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 16:50 +04 | 18 Apr 25 16:50 +04 |
| image      | load catalog-service:latest    | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 16:53 +04 |                     |
| image      | load catalog-service:latest    | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 17:08 +04 |                     |
| cache      | list                           | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 17:13 +04 | 18 Apr 25 17:13 +04 |
| cache      | delete catalog-service:latest  | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 17:13 +04 |                     |
| stop       |                                | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 17:14 +04 | 18 Apr 25 17:14 +04 |
| start      |                                | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 17:14 +04 | 18 Apr 25 17:15 +04 |
| image      | load catalog-service:latest    | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 17:19 +04 |                     |
| image      | load catalog-service:latest    | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 17:31 +04 |                     |
| image      | load my-java-image:1.0.0       | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 17:32 +04 |                     |
| image      | load my-java-image:1.0.0       | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 17:33 +04 |                     |
| cache      | delete catalog-service:latest  | minikube | DESKTOP-T8FR5QM\User | v1.32.0 | 18 Apr 25 17:35 +04 |                     |
|------------|--------------------------------|----------|----------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2025/04/18 17:14:26
Running on machine: DESKTOP-T8FR5QM
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0418 17:14:26.088374   27360 out.go:296] Setting OutFile to fd 108 ...
I0418 17:14:26.089484   27360 out.go:343] TERM=,COLORTERM=, which probably does not support color
I0418 17:14:26.089484   27360 out.go:309] Setting ErrFile to fd 112...
I0418 17:14:26.089484   27360 out.go:343] TERM=,COLORTERM=, which probably does not support color
I0418 17:14:26.089484   27360 oci.go:576] shell is pointing to dockerd inside minikube. will unset to use host
W0418 17:14:26.106071   27360 root.go:314] Error reading config file at C:\Users\User\.minikube\config\config.json: open C:\Users\User\.minikube\config\config.json: The system cannot find the file specified.
I0418 17:14:26.134213   27360 out.go:303] Setting JSON to false
I0418 17:14:26.142113   27360 start.go:128] hostinfo: {"hostname":"DESKTOP-T8FR5QM","uptime":717486,"bootTime":1744264580,"procs":307,"os":"windows","platform":"Microsoft Windows 11 Pro Education","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.3775 Build 26100.3775","kernelVersion":"10.0.26100.3775 Build 26100.3775","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"bc5a0cef-2781-4cec-b81f-8ff7d8bb9671"}
W0418 17:14:26.142113   27360 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0418 17:14:26.143792   27360 out.go:177] * minikube v1.32.0 on Microsoft Windows 11 Pro Education 10.0.26100.3775 Build 26100.3775
I0418 17:14:26.145406   27360 notify.go:220] Checking for updates...
I0418 17:14:26.145968   27360 out.go:177]   - MINIKUBE_ACTIVE_DOCKERD=minikube
I0418 17:14:26.148300   27360 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0418 17:14:26.148300   27360 driver.go:378] Setting default libvirt URI to qemu:///system
I0418 17:14:26.433571   27360 docker.go:122] docker version: linux-25.0.3:Docker Desktop 4.28.0 (139021)
I0418 17:14:26.437503   27360 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0418 17:14:32.788572   27360 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (6.3510687s)
I0418 17:14:32.790098   27360 info.go:266] docker info: {ID:0c18ed21-0800-4c35-88e3-b6736f0d6a50 Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:16 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:51 OomKillDisable:true NGoroutines:83 SystemTime:2025-04-18 13:14:32.679100927 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8191909888 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I0418 17:14:32.791333   27360 out.go:177] * Using the docker driver based on existing profile
I0418 17:14:32.793203   27360 start.go:298] selected driver: docker
I0418 17:14:32.793203   27360 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\User:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0418 17:14:32.793203   27360 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0418 17:14:32.809854   27360 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0418 17:14:33.455744   27360 info.go:266] docker info: {ID:0c18ed21-0800-4c35-88e3-b6736f0d6a50 Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:16 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:52 OomKillDisable:true NGoroutines:85 SystemTime:2025-04-18 13:14:33.397215874 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8191909888 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I0418 17:14:33.799657   27360 cni.go:84] Creating CNI manager for ""
I0418 17:14:33.799657   27360 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0418 17:14:33.799657   27360 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\User:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0418 17:14:33.801983   27360 out.go:177] * Starting control plane node minikube in cluster minikube
I0418 17:14:33.804471   27360 cache.go:121] Beginning downloading kic base image for docker with docker
I0418 17:14:33.805713   27360 out.go:177] * Pulling base image ...
I0418 17:14:33.807480   27360 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0418 17:14:33.807480   27360 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0418 17:14:33.810942   27360 preload.go:148] Found local preload: C:\Users\User\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0418 17:14:33.810942   27360 cache.go:56] Caching tarball of preloaded images
I0418 17:14:33.811615   27360 preload.go:174] Found C:\Users\User\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0418 17:14:33.811615   27360 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0418 17:14:33.812248   27360 profile.go:148] Saving config to C:\Users\User\.minikube\profiles\minikube\config.json ...
I0418 17:14:34.218005   27360 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0418 17:14:34.218005   27360 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0418 17:14:34.218005   27360 cache.go:194] Successfully downloaded all kic artifacts
I0418 17:14:34.218823   27360 start.go:365] acquiring machines lock for minikube: {Name:mk910e2d185d973fbe12ec8c7330b2d1d5ea153f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0418 17:14:34.218823   27360 start.go:369] acquired machines lock for "minikube" in 0s
I0418 17:14:34.218823   27360 start.go:96] Skipping create...Using existing machine configuration
I0418 17:14:34.218823   27360 fix.go:54] fixHost starting: 
I0418 17:14:34.229449   27360 out.go:177] * Noticed you have an activated docker-env on docker driver in this terminal:
W0418 17:14:34.234874   27360 out.go:239] ! Please re-eval your docker-env, To ensure your environment variables have updated ports:

	'minikube -p minikube docker-env'

	
I0418 17:14:34.246078   27360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0418 17:14:34.517311   27360 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0418 17:14:34.517311   27360 fix.go:128] unexpected machine state, will restart: <nil>
I0418 17:14:34.519297   27360 out.go:177] * Restarting existing docker container for "minikube" ...
I0418 17:14:34.524950   27360 cli_runner.go:164] Run: docker start minikube
I0418 17:14:35.784255   27360 cli_runner.go:217] Completed: docker start minikube: (1.2593045s)
I0418 17:14:35.788644   27360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0418 17:14:36.037838   27360 kic.go:430] container "minikube" state is running.
I0418 17:14:36.045026   27360 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0418 17:14:36.339289   27360 profile.go:148] Saving config to C:\Users\User\.minikube\profiles\minikube\config.json ...
I0418 17:14:36.341382   27360 machine.go:88] provisioning docker machine ...
I0418 17:14:36.341382   27360 ubuntu.go:169] provisioning hostname "minikube"
I0418 17:14:36.346077   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:14:36.606231   27360 main.go:141] libmachine: Using SSH client type: native
I0418 17:14:36.607394   27360 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12047e0] 0x1207320 <nil>  [] 0s} 127.0.0.1 61698 <nil> <nil>}
I0418 17:14:36.607394   27360 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0418 17:14:37.135218   27360 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0418 17:14:37.156702   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:14:37.556745   27360 main.go:141] libmachine: Using SSH client type: native
I0418 17:14:37.558524   27360 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12047e0] 0x1207320 <nil>  [] 0s} 127.0.0.1 61698 <nil> <nil>}
I0418 17:14:37.558524   27360 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0418 17:14:37.921409   27360 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0418 17:14:37.921409   27360 ubuntu.go:175] set auth options {CertDir:C:\Users\User\.minikube CaCertPath:C:\Users\User\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\User\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\User\.minikube\machines\server.pem ServerKeyPath:C:\Users\User\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\User\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\User\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\User\.minikube}
I0418 17:14:37.921409   27360 ubuntu.go:177] setting up certificates
I0418 17:14:37.921409   27360 provision.go:83] configureAuth start
I0418 17:14:37.931756   27360 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0418 17:14:38.307297   27360 provision.go:138] copyHostCerts
I0418 17:14:38.308633   27360 exec_runner.go:144] found C:\Users\User\.minikube/ca.pem, removing ...
I0418 17:14:38.308633   27360 exec_runner.go:203] rm: C:\Users\User\.minikube\ca.pem
I0418 17:14:38.309518   27360 exec_runner.go:151] cp: C:\Users\User\.minikube\certs\ca.pem --> C:\Users\User\.minikube/ca.pem (1070 bytes)
I0418 17:14:38.313132   27360 exec_runner.go:144] found C:\Users\User\.minikube/cert.pem, removing ...
I0418 17:14:38.313132   27360 exec_runner.go:203] rm: C:\Users\User\.minikube\cert.pem
I0418 17:14:38.314191   27360 exec_runner.go:151] cp: C:\Users\User\.minikube\certs\cert.pem --> C:\Users\User\.minikube/cert.pem (1115 bytes)
I0418 17:14:38.317796   27360 exec_runner.go:144] found C:\Users\User\.minikube/key.pem, removing ...
I0418 17:14:38.317796   27360 exec_runner.go:203] rm: C:\Users\User\.minikube\key.pem
I0418 17:14:38.318191   27360 exec_runner.go:151] cp: C:\Users\User\.minikube\certs\key.pem --> C:\Users\User\.minikube/key.pem (1675 bytes)
I0418 17:14:38.320717   27360 provision.go:112] generating server cert: C:\Users\User\.minikube\machines\server.pem ca-key=C:\Users\User\.minikube\certs\ca.pem private-key=C:\Users\User\.minikube\certs\ca-key.pem org=User.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0418 17:14:38.562655   27360 provision.go:172] copyRemoteCerts
I0418 17:14:38.568579   27360 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0418 17:14:38.578297   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:14:38.938160   27360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61698 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0418 17:14:39.096175   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0418 17:14:39.203674   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I0418 17:14:39.298949   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0418 17:14:39.366345   27360 provision.go:86] duration metric: configureAuth took 1.4449367s
I0418 17:14:39.366345   27360 ubuntu.go:193] setting minikube options for container-runtime
I0418 17:14:39.370239   27360 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0418 17:14:39.380272   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:14:39.724821   27360 main.go:141] libmachine: Using SSH client type: native
I0418 17:14:39.725550   27360 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12047e0] 0x1207320 <nil>  [] 0s} 127.0.0.1 61698 <nil> <nil>}
I0418 17:14:39.725550   27360 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0418 17:14:39.952768   27360 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0418 17:14:39.952768   27360 ubuntu.go:71] root file system type: overlay
I0418 17:14:39.953351   27360 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0418 17:14:39.964003   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:14:40.289076   27360 main.go:141] libmachine: Using SSH client type: native
I0418 17:14:40.289771   27360 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12047e0] 0x1207320 <nil>  [] 0s} 127.0.0.1 61698 <nil> <nil>}
I0418 17:14:40.290487   27360 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0418 17:14:40.535765   27360 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0418 17:14:40.546653   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:14:40.866092   27360 main.go:141] libmachine: Using SSH client type: native
I0418 17:14:40.867196   27360 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12047e0] 0x1207320 <nil>  [] 0s} 127.0.0.1 61698 <nil> <nil>}
I0418 17:14:40.867196   27360 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0418 17:14:41.105211   27360 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0418 17:14:41.105211   27360 machine.go:91] provisioned docker machine in 4.7638295s
I0418 17:14:41.105211   27360 start.go:300] post-start starting for "minikube" (driver="docker")
I0418 17:14:41.105211   27360 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0418 17:14:41.108548   27360 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0418 17:14:41.117064   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:14:41.463982   27360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61698 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0418 17:14:41.743326   27360 ssh_runner.go:195] Run: cat /etc/os-release
I0418 17:14:41.753440   27360 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0418 17:14:41.753440   27360 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0418 17:14:41.753440   27360 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0418 17:14:41.753440   27360 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0418 17:14:41.754122   27360 filesync.go:126] Scanning C:\Users\User\.minikube\addons for local assets ...
I0418 17:14:41.754637   27360 filesync.go:126] Scanning C:\Users\User\.minikube\files for local assets ...
I0418 17:14:41.755277   27360 start.go:303] post-start completed in 650.0653ms
I0418 17:14:41.774644   27360 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0418 17:14:41.778958   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:14:42.122482   27360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61698 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0418 17:14:42.349725   27360 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0418 17:14:42.360947   27360 fix.go:56] fixHost completed within 8.1421237s
I0418 17:14:42.360947   27360 start.go:83] releasing machines lock for "minikube", held for 8.1421237s
I0418 17:14:42.367322   27360 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0418 17:14:42.654822   27360 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0418 17:14:42.765415   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:14:42.789509   27360 ssh_runner.go:195] Run: cat /version.json
I0418 17:14:42.807291   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:14:43.100355   27360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61698 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0418 17:14:43.105636   27360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61698 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0418 17:14:45.373018   27360 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.7181956s)
W0418 17:14:45.373018   27360 start.go:840] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Operation timed out after 2001 milliseconds with 0 bytes received
W0418 17:14:45.373018   27360 out.go:239] ! This container is having trouble accessing https://registry.k8s.io
I0418 17:14:45.373018   27360 ssh_runner.go:235] Completed: cat /version.json: (2.583509s)
W0418 17:14:45.381788   27360 out.go:239] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0418 17:14:45.476140   27360 ssh_runner.go:195] Run: systemctl --version
I0418 17:14:45.515600   27360 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0418 17:14:45.534712   27360 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0418 17:14:45.562494   27360 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0418 17:14:45.567558   27360 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0418 17:14:45.618178   27360 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0418 17:14:45.618178   27360 start.go:472] detecting cgroup driver to use...
I0418 17:14:45.618178   27360 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0418 17:14:45.618178   27360 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0418 17:14:45.771946   27360 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0418 17:14:45.829117   27360 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0418 17:14:45.867835   27360 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0418 17:14:45.912086   27360 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0418 17:14:45.958715   27360 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0418 17:14:46.047484   27360 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0418 17:14:46.096874   27360 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0418 17:14:46.129099   27360 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0418 17:14:46.174926   27360 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0418 17:14:46.207280   27360 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0418 17:14:46.246592   27360 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0418 17:14:46.265240   27360 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0418 17:14:46.481259   27360 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0418 17:14:46.652167   27360 start.go:472] detecting cgroup driver to use...
I0418 17:14:46.652167   27360 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0418 17:14:46.655234   27360 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0418 17:14:46.677476   27360 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0418 17:14:46.679845   27360 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0418 17:14:46.699958   27360 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0418 17:14:46.864083   27360 ssh_runner.go:195] Run: which cri-dockerd
I0418 17:14:46.873902   27360 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0418 17:14:46.885189   27360 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0418 17:14:46.952973   27360 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0418 17:14:47.181148   27360 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0418 17:14:47.325555   27360 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0418 17:14:47.325555   27360 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0418 17:14:47.363951   27360 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0418 17:14:47.589739   27360 ssh_runner.go:195] Run: sudo systemctl restart docker
I0418 17:14:48.767349   27360 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.1776107s)
I0418 17:14:48.769447   27360 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0418 17:14:48.940506   27360 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0418 17:14:49.068605   27360 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0418 17:14:49.223104   27360 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0418 17:14:49.388000   27360 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0418 17:14:49.441059   27360 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0418 17:14:49.611329   27360 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0418 17:14:49.889782   27360 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0418 17:14:49.899650   27360 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0418 17:14:49.916278   27360 start.go:540] Will wait 60s for crictl version
I0418 17:14:49.931943   27360 ssh_runner.go:195] Run: which crictl
I0418 17:14:49.955221   27360 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0418 17:14:50.221427   27360 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0418 17:14:50.224135   27360 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0418 17:14:50.285459   27360 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0418 17:14:50.344689   27360 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0418 17:14:50.349607   27360 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0418 17:14:50.761495   27360 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0418 17:14:50.777004   27360 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0418 17:14:50.783783   27360 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0418 17:14:50.800561   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0418 17:14:51.023248   27360 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0418 17:14:51.027727   27360 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0418 17:14:51.112410   27360 docker.go:671] Got preloaded images: -- stdout --
catalog-service:latest
postgres:14.12
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
<none>:<none>

-- /stdout --
I0418 17:14:51.112410   27360 docker.go:601] Images already preloaded, skipping extraction
I0418 17:14:51.116898   27360 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0418 17:14:51.179270   27360 docker.go:671] Got preloaded images: -- stdout --
catalog-service:latest
postgres:14.12
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
<none>:<none>

-- /stdout --
I0418 17:14:51.179270   27360 cache_images.go:84] Images are preloaded, skipping loading
I0418 17:14:51.184849   27360 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0418 17:14:51.512317   27360 cni.go:84] Creating CNI manager for ""
I0418 17:14:51.512317   27360 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0418 17:14:51.512317   27360 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0418 17:14:51.512317   27360 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0418 17:14:51.512838   27360 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0418 17:14:51.512838   27360 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0418 17:14:51.514189   27360 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0418 17:14:51.571384   27360 binaries.go:44] Found k8s binaries, skipping transfer
I0418 17:14:51.574131   27360 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0418 17:14:51.599896   27360 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0418 17:14:51.647147   27360 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0418 17:14:51.701322   27360 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0418 17:14:51.752639   27360 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0418 17:14:51.780536   27360 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0418 17:14:51.804282   27360 certs.go:56] Setting up C:\Users\User\.minikube\profiles\minikube for IP: 192.168.49.2
I0418 17:14:51.804282   27360 certs.go:190] acquiring lock for shared ca certs: {Name:mk0fa5de13b5935dba1bd738b050f4c2e4c5789d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0418 17:14:51.805933   27360 certs.go:199] skipping minikubeCA CA generation: C:\Users\User\.minikube\ca.key
I0418 17:14:51.807272   27360 certs.go:199] skipping proxyClientCA CA generation: C:\Users\User\.minikube\proxy-client-ca.key
I0418 17:14:51.807928   27360 certs.go:315] skipping minikube-user signed cert generation: C:\Users\User\.minikube\profiles\minikube\client.key
I0418 17:14:51.809648   27360 certs.go:315] skipping minikube signed cert generation: C:\Users\User\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0418 17:14:51.810930   27360 certs.go:315] skipping aggregator signed cert generation: C:\Users\User\.minikube\profiles\minikube\proxy-client.key
I0418 17:14:51.813667   27360 certs.go:437] found cert: C:\Users\User\.minikube\certs\C:\Users\User\.minikube\certs\ca-key.pem (1675 bytes)
I0418 17:14:51.813667   27360 certs.go:437] found cert: C:\Users\User\.minikube\certs\C:\Users\User\.minikube\certs\ca.pem (1070 bytes)
I0418 17:14:51.813667   27360 certs.go:437] found cert: C:\Users\User\.minikube\certs\C:\Users\User\.minikube\certs\cert.pem (1115 bytes)
I0418 17:14:51.814406   27360 certs.go:437] found cert: C:\Users\User\.minikube\certs\C:\Users\User\.minikube\certs\key.pem (1675 bytes)
I0418 17:14:51.815241   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0418 17:14:51.868462   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0418 17:14:51.915944   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0418 17:14:51.974713   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0418 17:14:52.030220   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0418 17:14:52.085068   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0418 17:14:52.138373   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0418 17:14:52.177546   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0418 17:14:52.227645   27360 ssh_runner.go:362] scp C:\Users\User\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0418 17:14:52.296460   27360 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0418 17:14:52.374012   27360 ssh_runner.go:195] Run: openssl version
I0418 17:14:52.422338   27360 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0418 17:14:52.476196   27360 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0418 17:14:52.485504   27360 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Apr 29  2024 /usr/share/ca-certificates/minikubeCA.pem
I0418 17:14:52.496513   27360 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0418 17:14:52.508822   27360 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0418 17:14:52.544150   27360 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0418 17:14:52.571829   27360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0418 17:14:52.609306   27360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0418 17:14:52.640559   27360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0418 17:14:52.673376   27360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0418 17:14:52.708470   27360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0418 17:14:52.729791   27360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0418 17:14:52.749315   27360 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\User:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0418 17:14:52.754502   27360 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0418 17:14:52.807511   27360 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0418 17:14:52.864206   27360 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0418 17:14:52.864206   27360 kubeadm.go:636] restartCluster start
I0418 17:14:52.866428   27360 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0418 17:14:52.909015   27360 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0418 17:14:52.914301   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0418 17:14:53.118835   27360 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in C:\Users\User\.kube\config
I0418 17:14:53.119461   27360 kubeconfig.go:146] "minikube" context is missing from C:\Users\User\.kube\config - will repair!
I0418 17:14:53.121279   27360 lock.go:35] WriteFile acquiring C:\Users\User\.kube\config: {Name:mk4c22283734d3c4206a5f8b8d3853863fcc1163 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0418 17:14:53.173289   27360 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0418 17:14:53.185695   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:53.187339   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:53.205306   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:53.205306   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:53.207149   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:53.230037   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:53.731730   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:53.733243   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:53.780978   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:54.242379   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:54.245242   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:54.268014   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:54.732245   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:54.733645   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:54.813145   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:55.234240   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:55.234991   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:55.268330   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:55.744565   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:55.745790   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:55.775955   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:56.234816   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:56.236319   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:56.270170   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:56.736447   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:56.737586   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:56.836170   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:57.243521   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:57.244339   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:57.292940   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:57.732256   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:57.733407   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:57.757107   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:58.234060   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:58.234567   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:58.252659   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:58.731823   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:58.733362   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:58.772929   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:59.233731   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:59.236154   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:59.261567   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:14:59.736068   27360 api_server.go:166] Checking apiserver status ...
I0418 17:14:59.737518   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:14:59.754393   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:15:00.237144   27360 api_server.go:166] Checking apiserver status ...
I0418 17:15:00.237819   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:15:00.250930   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:15:00.735667   27360 api_server.go:166] Checking apiserver status ...
I0418 17:15:00.737449   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:15:00.757055   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:15:01.230909   27360 api_server.go:166] Checking apiserver status ...
I0418 17:15:01.230909   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:15:01.282527   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:15:01.741033   27360 api_server.go:166] Checking apiserver status ...
I0418 17:15:01.742369   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:15:01.781968   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:15:02.236257   27360 api_server.go:166] Checking apiserver status ...
I0418 17:15:02.237578   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:15:02.259489   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:15:02.737428   27360 api_server.go:166] Checking apiserver status ...
I0418 17:15:02.738131   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0418 17:15:02.778066   27360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0418 17:15:03.186961   27360 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0418 17:15:03.186961   27360 kubeadm.go:1128] stopping kube-system containers ...
I0418 17:15:03.193792   27360 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0418 17:15:03.266934   27360 docker.go:469] Stopping containers: [c64afb849f0a 43c9605dd8d2 9ea059f6fa8f 2cf2978970a2 7ffcec4c346e f0b4c228657c 30773ac0918f f4aad0be47e9 30f7522c2548 327cc35e80df 62da2bb5fb6f e40a24ea2310 e7b2d4216a14 97700cbbb8f6 84abb21316ff 20a1e02ab389 a889c690bf32 9a446ccd7a17 27761829f3a7 d700ff4f90fb ef700d616380 b355c517963e 7acbf9fa1bec 71d5dd5c27e0 c526f0d30918 d11a94634b7f 349edbfc46ae]
I0418 17:15:03.270619   27360 ssh_runner.go:195] Run: docker stop c64afb849f0a 43c9605dd8d2 9ea059f6fa8f 2cf2978970a2 7ffcec4c346e f0b4c228657c 30773ac0918f f4aad0be47e9 30f7522c2548 327cc35e80df 62da2bb5fb6f e40a24ea2310 e7b2d4216a14 97700cbbb8f6 84abb21316ff 20a1e02ab389 a889c690bf32 9a446ccd7a17 27761829f3a7 d700ff4f90fb ef700d616380 b355c517963e 7acbf9fa1bec 71d5dd5c27e0 c526f0d30918 d11a94634b7f 349edbfc46ae
I0418 17:15:03.357467   27360 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0418 17:15:03.408767   27360 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0418 17:15:03.465293   27360 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Apr 18 11:32 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Apr 18 11:32 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5659 Apr 18 11:32 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Apr 18 11:32 /etc/kubernetes/scheduler.conf

I0418 17:15:03.467600   27360 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0418 17:15:03.539029   27360 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0418 17:15:03.584286   27360 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0418 17:15:03.652246   27360 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0418 17:15:03.654255   27360 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0418 17:15:03.706992   27360 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0418 17:15:03.728769   27360 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0418 17:15:03.730994   27360 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0418 17:15:03.763755   27360 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0418 17:15:03.787313   27360 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0418 17:15:03.787313   27360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0418 17:15:04.125545   27360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0418 17:15:05.684544   27360 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.5589994s)
I0418 17:15:05.684544   27360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0418 17:15:05.950509   27360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0418 17:15:06.052252   27360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0418 17:15:06.144082   27360 api_server.go:52] waiting for apiserver process to appear ...
I0418 17:15:06.145123   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:06.203797   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:06.733164   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:07.231866   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:07.730530   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:08.231978   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:08.730975   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:09.226645   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:09.729306   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:10.239278   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:10.342205   27360 api_server.go:72] duration metric: took 4.1981229s to wait for apiserver process to appear ...
I0418 17:15:10.342205   27360 api_server.go:88] waiting for apiserver healthz status ...
I0418 17:15:10.342205   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:10.372080   27360 api_server.go:269] stopped: https://127.0.0.1:61697/healthz: Get "https://127.0.0.1:61697/healthz": EOF
I0418 17:15:10.372080   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:10.376822   27360 api_server.go:269] stopped: https://127.0.0.1:61697/healthz: Get "https://127.0.0.1:61697/healthz": EOF
I0418 17:15:10.880472   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:10.885743   27360 api_server.go:269] stopped: https://127.0.0.1:61697/healthz: Get "https://127.0.0.1:61697/healthz": EOF
I0418 17:15:11.388866   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:11.394074   27360 api_server.go:269] stopped: https://127.0.0.1:61697/healthz: Get "https://127.0.0.1:61697/healthz": EOF
I0418 17:15:11.885286   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:11.891721   27360 api_server.go:269] stopped: https://127.0.0.1:61697/healthz: Get "https://127.0.0.1:61697/healthz": EOF
I0418 17:15:12.380915   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:12.386239   27360 api_server.go:269] stopped: https://127.0.0.1:61697/healthz: Get "https://127.0.0.1:61697/healthz": EOF
I0418 17:15:12.889043   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:17.902087   27360 api_server.go:269] stopped: https://127.0.0.1:61697/healthz: Get "https://127.0.0.1:61697/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0418 17:15:17.902087   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:18.238691   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0418 17:15:18.238691   27360 api_server.go:103] status: https://127.0.0.1:61697/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0418 17:15:18.386398   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:18.438676   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0418 17:15:18.438676   27360 api_server.go:103] status: https://127.0.0.1:61697/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0418 17:15:18.878966   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:18.938341   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0418 17:15:18.938341   27360 api_server.go:103] status: https://127.0.0.1:61697/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0418 17:15:19.381184   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:19.432100   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0418 17:15:19.432100   27360 api_server.go:103] status: https://127.0.0.1:61697/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0418 17:15:19.885881   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:19.937931   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0418 17:15:19.937931   27360 api_server.go:103] status: https://127.0.0.1:61697/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0418 17:15:20.383948   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:20.433695   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0418 17:15:20.434287   27360 api_server.go:103] status: https://127.0.0.1:61697/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0418 17:15:20.891548   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:20.937766   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0418 17:15:20.937766   27360 api_server.go:103] status: https://127.0.0.1:61697/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0418 17:15:21.385351   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:21.435331   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0418 17:15:21.435331   27360 api_server.go:103] status: https://127.0.0.1:61697/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0418 17:15:21.880560   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:22.022611   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0418 17:15:22.022611   27360 api_server.go:103] status: https://127.0.0.1:61697/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0418 17:15:22.389180   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:22.444738   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 200:
ok
I0418 17:15:22.627375   27360 api_server.go:141] control plane version: v1.28.3
I0418 17:15:22.628771   27360 api_server.go:131] duration metric: took 12.2859448s to wait for apiserver health ...
I0418 17:15:22.628771   27360 cni.go:84] Creating CNI manager for ""
I0418 17:15:22.629362   27360 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0418 17:15:22.630112   27360 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I0418 17:15:22.636791   27360 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0418 17:15:22.930953   27360 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0418 17:15:23.328607   27360 system_pods.go:43] waiting for kube-system pods to appear ...
I0418 17:15:23.442285   27360 system_pods.go:59] 7 kube-system pods found
I0418 17:15:23.442285   27360 system_pods.go:61] "coredns-5dd5756b68-gc7x7" [3c12b299-f88b-47d6-a487-aff6d9cd1677] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0418 17:15:23.442285   27360 system_pods.go:61] "etcd-minikube" [14517dc8-09ce-4fe9-bae9-61baa7ca3e8c] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0418 17:15:23.442285   27360 system_pods.go:61] "kube-apiserver-minikube" [b51a39ac-dfcc-4590-b3fb-a8644333a6dd] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0418 17:15:23.442285   27360 system_pods.go:61] "kube-controller-manager-minikube" [1ee8c235-8011-44e2-8f03-4230ce0e4552] Running
I0418 17:15:23.442285   27360 system_pods.go:61] "kube-proxy-zvrvl" [7a175877-5dde-4176-82e8-d50be8d1dde9] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0418 17:15:23.442285   27360 system_pods.go:61] "kube-scheduler-minikube" [81c9d69b-a14a-4e6e-a013-6d3135f79a23] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0418 17:15:23.442285   27360 system_pods.go:61] "storage-provisioner" [c849d09f-5ec6-46bd-a57e-eea33b6cf875] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0418 17:15:23.442285   27360 system_pods.go:74] duration metric: took 113.6778ms to wait for pod list to return data ...
I0418 17:15:23.442285   27360 node_conditions.go:102] verifying NodePressure condition ...
I0418 17:15:23.632659   27360 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0418 17:15:23.632659   27360 node_conditions.go:123] node cpu capacity is 16
I0418 17:15:23.632659   27360 node_conditions.go:105] duration metric: took 190.3747ms to run NodePressure ...
I0418 17:15:23.632659   27360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0418 17:15:27.837410   27360 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (4.2047504s)
I0418 17:15:27.837410   27360 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0418 17:15:28.140631   27360 ops.go:34] apiserver oom_adj: -16
I0418 17:15:28.140631   27360 kubeadm.go:640] restartCluster took 35.2764251s
I0418 17:15:28.140631   27360 kubeadm.go:406] StartCluster complete in 35.3913163s
I0418 17:15:28.140631   27360 settings.go:142] acquiring lock: {Name:mkbb158d68fd483b3bca5b8a92ada72bb65452d7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0418 17:15:28.140631   27360 settings.go:150] Updating kubeconfig:  C:\Users\User\.kube\config
I0418 17:15:28.143095   27360 lock.go:35] WriteFile acquiring C:\Users\User\.kube\config: {Name:mk4c22283734d3c4206a5f8b8d3853863fcc1163 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0418 17:15:28.145129   27360 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0418 17:15:28.145535   27360 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0418 17:15:28.145535   27360 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0418 17:15:28.145535   27360 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0418 17:15:28.145535   27360 addons.go:240] addon storage-provisioner should already be in state true
I0418 17:15:28.146132   27360 addons.go:69] Setting dashboard=true in profile "minikube"
I0418 17:15:28.146132   27360 addons.go:231] Setting addon dashboard=true in "minikube"
W0418 17:15:28.146132   27360 addons.go:240] addon dashboard should already be in state true
I0418 17:15:28.146132   27360 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0418 17:15:28.146732   27360 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0418 17:15:28.146732   27360 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0418 17:15:28.146732   27360 host.go:66] Checking if "minikube" exists ...
I0418 17:15:28.146732   27360 host.go:66] Checking if "minikube" exists ...
I0418 17:15:28.168999   27360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0418 17:15:28.168999   27360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0418 17:15:28.170608   27360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0418 17:15:28.429599   27360 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0418 17:15:28.429599   27360 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0418 17:15:28.431495   27360 out.go:177] * Verifying Kubernetes components...
I0418 17:15:28.436152   27360 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0418 17:15:28.457536   27360 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0418 17:15:28.457536   27360 addons.go:240] addon default-storageclass should already be in state true
I0418 17:15:28.458135   27360 host.go:66] Checking if "minikube" exists ...
I0418 17:15:28.474108   27360 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0418 17:15:28.475232   27360 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0418 17:15:28.475804   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0418 17:15:28.480123   27360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0418 17:15:28.484044   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:15:28.494091   27360 out.go:177]   - Using image docker.io/kubernetesui/dashboard:v2.7.0
I0418 17:15:28.496417   27360 out.go:177]   - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0418 17:15:28.498892   27360 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0418 17:15:28.498892   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0418 17:15:28.506535   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:15:28.782916   27360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61698 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0418 17:15:28.798610   27360 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0418 17:15:28.798610   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0418 17:15:28.808795   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0418 17:15:28.821254   27360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61698 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0418 17:15:29.267146   27360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61698 SSHKeyPath:C:\Users\User\.minikube\machines\minikube\id_rsa Username:docker}
I0418 17:15:30.234772   27360 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0418 17:15:30.325303   27360 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0418 17:15:30.325303   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0418 17:15:30.824222   27360 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0418 17:15:30.824222   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0418 17:15:30.834770   27360 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0418 17:15:31.430068   27360 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0418 17:15:31.430068   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0418 17:15:31.930908   27360 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0418 17:15:31.930908   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0418 17:15:32.530574   27360 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0418 17:15:32.530574   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0418 17:15:33.129036   27360 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0418 17:15:33.129036   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0418 17:15:33.930139   27360 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0418 17:15:33.930139   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0418 17:15:34.826275   27360 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0418 17:15:34.826275   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0418 17:15:35.327365   27360 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0418 17:15:35.327365   27360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0418 17:15:35.936712   27360 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0418 17:15:36.129556   27360 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (7.9844268s)
I0418 17:15:36.130129   27360 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (7.6939769s)
I0418 17:15:36.130129   27360 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0418 17:15:36.140186   27360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0418 17:15:36.565515   27360 api_server.go:52] waiting for apiserver process to appear ...
I0418 17:15:36.567178   27360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0418 17:15:48.729504   27360 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (18.494732s)
I0418 17:15:48.730158   27360 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (17.8953879s)
I0418 17:15:51.655163   27360 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (15.7184513s)
I0418 17:15:51.655163   27360 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (15.0879853s)
I0418 17:15:51.656788   27360 out.go:177] * Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0418 17:15:51.655163   27360 api_server.go:72] duration metric: took 23.2255635s to wait for apiserver process to appear ...
I0418 17:15:51.656788   27360 api_server.go:88] waiting for apiserver healthz status ...
I0418 17:15:51.656788   27360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61697/healthz ...
I0418 17:15:51.659292   27360 out.go:177] * Enabled addons: storage-provisioner, default-storageclass, dashboard
I0418 17:15:51.660459   27360 addons.go:502] enable addons completed in 23.5149247s: enabled=[storage-provisioner default-storageclass dashboard]
I0418 17:15:51.742189   27360 api_server.go:279] https://127.0.0.1:61697/healthz returned 200:
ok
I0418 17:15:51.749315   27360 api_server.go:141] control plane version: v1.28.3
I0418 17:15:51.749315   27360 api_server.go:131] duration metric: took 92.5272ms to wait for apiserver health ...
I0418 17:15:51.749315   27360 system_pods.go:43] waiting for kube-system pods to appear ...
I0418 17:15:51.836885   27360 system_pods.go:59] 7 kube-system pods found
I0418 17:15:51.836885   27360 system_pods.go:61] "coredns-5dd5756b68-gc7x7" [3c12b299-f88b-47d6-a487-aff6d9cd1677] Running
I0418 17:15:51.836885   27360 system_pods.go:61] "etcd-minikube" [14517dc8-09ce-4fe9-bae9-61baa7ca3e8c] Running
I0418 17:15:51.836885   27360 system_pods.go:61] "kube-apiserver-minikube" [b51a39ac-dfcc-4590-b3fb-a8644333a6dd] Running
I0418 17:15:51.836885   27360 system_pods.go:61] "kube-controller-manager-minikube" [1ee8c235-8011-44e2-8f03-4230ce0e4552] Running
I0418 17:15:51.836885   27360 system_pods.go:61] "kube-proxy-zvrvl" [7a175877-5dde-4176-82e8-d50be8d1dde9] Running
I0418 17:15:51.836885   27360 system_pods.go:61] "kube-scheduler-minikube" [81c9d69b-a14a-4e6e-a013-6d3135f79a23] Running
I0418 17:15:51.837543   27360 system_pods.go:61] "storage-provisioner" [c849d09f-5ec6-46bd-a57e-eea33b6cf875] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0418 17:15:51.837543   27360 system_pods.go:74] duration metric: took 88.2279ms to wait for pod list to return data ...
I0418 17:15:51.837543   27360 kubeadm.go:581] duration metric: took 23.4079435s to wait for : map[apiserver:true system_pods:true] ...
I0418 17:15:51.837543   27360 node_conditions.go:102] verifying NodePressure condition ...
I0418 17:15:51.847874   27360 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0418 17:15:51.847874   27360 node_conditions.go:123] node cpu capacity is 16
I0418 17:15:51.847874   27360 node_conditions.go:105] duration metric: took 10.3308ms to run NodePressure ...
I0418 17:15:51.847874   27360 start.go:228] waiting for startup goroutines ...
I0418 17:15:51.847874   27360 start.go:233] waiting for cluster config update ...
I0418 17:15:51.847874   27360 start.go:242] writing updated cluster config ...
I0418 17:15:51.878904   27360 ssh_runner.go:195] Run: rm -f paused
I0418 17:15:52.219312   27360 start.go:600] kubectl: 1.29.1, cluster: 1.28.3 (minor skew: 1)
I0418 17:15:52.220680   27360 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Apr 18 13:14:47 minikube systemd[1]: docker.service: Deactivated successfully.
Apr 18 13:14:47 minikube systemd[1]: Stopped Docker Application Container Engine.
Apr 18 13:14:47 minikube systemd[1]: Starting Docker Application Container Engine...
Apr 18 13:14:47 minikube dockerd[965]: time="2025-04-18T13:14:47.762652114Z" level=info msg="Starting up"
Apr 18 13:14:47 minikube dockerd[965]: time="2025-04-18T13:14:47.791275672Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Apr 18 13:14:47 minikube dockerd[965]: time="2025-04-18T13:14:47.815932959Z" level=info msg="Loading containers: start."
Apr 18 13:14:48 minikube dockerd[965]: time="2025-04-18T13:14:48.375886073Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Apr 18 13:14:48 minikube dockerd[965]: time="2025-04-18T13:14:48.443075626Z" level=info msg="Loading containers: done."
Apr 18 13:14:48 minikube dockerd[965]: time="2025-04-18T13:14:48.469795239Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Apr 18 13:14:48 minikube dockerd[965]: time="2025-04-18T13:14:48.469876348Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Apr 18 13:14:48 minikube dockerd[965]: time="2025-04-18T13:14:48.469888145Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Apr 18 13:14:48 minikube dockerd[965]: time="2025-04-18T13:14:48.469896691Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Apr 18 13:14:48 minikube dockerd[965]: time="2025-04-18T13:14:48.469930760Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Apr 18 13:14:48 minikube dockerd[965]: time="2025-04-18T13:14:48.470063372Z" level=info msg="Daemon has completed initialization"
Apr 18 13:14:48 minikube dockerd[965]: time="2025-04-18T13:14:48.750083982Z" level=info msg="API listen on /var/run/docker.sock"
Apr 18 13:14:48 minikube dockerd[965]: time="2025-04-18T13:14:48.750106662Z" level=info msg="API listen on [::]:2376"
Apr 18 13:14:48 minikube systemd[1]: Started Docker Application Container Engine.
Apr 18 13:14:49 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Apr 18 13:14:49 minikube cri-dockerd[1219]: time="2025-04-18T13:14:49Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Apr 18 13:14:49 minikube cri-dockerd[1219]: time="2025-04-18T13:14:49Z" level=info msg="Start docker client with request timeout 0s"
Apr 18 13:14:49 minikube cri-dockerd[1219]: time="2025-04-18T13:14:49Z" level=info msg="Hairpin mode is set to hairpin-veth"
Apr 18 13:14:49 minikube cri-dockerd[1219]: time="2025-04-18T13:14:49Z" level=info msg="Loaded network plugin cni"
Apr 18 13:14:49 minikube cri-dockerd[1219]: time="2025-04-18T13:14:49Z" level=info msg="Docker cri networking managed by network plugin cni"
Apr 18 13:14:49 minikube cri-dockerd[1219]: time="2025-04-18T13:14:49Z" level=info msg="Docker Info: &{ID:4c49ff5e-cfb2-40da-a5a3-abf2fbd781a4 Containers:39 ContainersRunning:0 ContainersPaused:0 ContainersStopped:39 Images:13 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:36 SystemTime:2025-04-18T13:14:49.873287158Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc000394070 NCPU:16 MemTotal:8191909888 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Apr 18 13:14:49 minikube cri-dockerd[1219]: time="2025-04-18T13:14:49Z" level=info msg="Setting cgroupDriver cgroupfs"
Apr 18 13:14:49 minikube cri-dockerd[1219]: time="2025-04-18T13:14:49Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Apr 18 13:14:49 minikube cri-dockerd[1219]: time="2025-04-18T13:14:49Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Apr 18 13:14:49 minikube cri-dockerd[1219]: time="2025-04-18T13:14:49Z" level=info msg="Start cri-dockerd grpc backend"
Apr 18 13:14:49 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Apr 18 13:15:06 minikube cri-dockerd[1219]: time="2025-04-18T13:15:06Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"polar-postgres-5c854695c8-jwfdh_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"04da9d95f3fd38ae44aba42c6f3564042be72520b4530fc0ff73aa98ed69b040\""
Apr 18 13:15:06 minikube cri-dockerd[1219]: time="2025-04-18T13:15:06Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-8694d4445c-9pwp4_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"942965bbe7a16634d1e0702efeb6c2e30f59395768e19003462e0a292e391fa5\""
Apr 18 13:15:06 minikube cri-dockerd[1219]: time="2025-04-18T13:15:06Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-8694d4445c-9pwp4_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4d4114caca91ff91e30d5ff1a6caea3485371a2b4c8e86f4f5b2e217735386e1\""
Apr 18 13:15:06 minikube cri-dockerd[1219]: time="2025-04-18T13:15:06Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-p7rrs_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4ebc1ed1916d754a9634a8f1750d5497015cdddc13098d951cde21de4a48f289\""
Apr 18 13:15:06 minikube cri-dockerd[1219]: time="2025-04-18T13:15:06Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-p7rrs_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c7c816cfeea19da25e2e84a75ee5b96e2049cb1726b6e344be13884bd721394c\""
Apr 18 13:15:06 minikube cri-dockerd[1219]: time="2025-04-18T13:15:06Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"catalog-service-6f5d4b4f76-966d9_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fefe0f2e13f5be2c413e0516107c1eab3f47804d8ca030f1152d95373b641256\""
Apr 18 13:15:06 minikube cri-dockerd[1219]: time="2025-04-18T13:15:06Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-gc7x7_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9ea059f6fa8fc65f20f7e86939c868fde17ed4b8aacd8f2e64b4df57c0f78215\""
Apr 18 13:15:06 minikube cri-dockerd[1219]: time="2025-04-18T13:15:06Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-gc7x7_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a889c690bf32a873d4cd333310ad35a8946598bdf0c77cef4a56354bc4641718\""
Apr 18 13:15:07 minikube cri-dockerd[1219]: time="2025-04-18T13:15:07Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"d11a94634b7fede804f977e90d118cf5e07a4dfae6adbd895932a8f8f5bbefa5\". Proceed without further sandbox information."
Apr 18 13:15:08 minikube cri-dockerd[1219]: time="2025-04-18T13:15:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-gc7x7_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9ea059f6fa8fc65f20f7e86939c868fde17ed4b8aacd8f2e64b4df57c0f78215\""
Apr 18 13:15:08 minikube cri-dockerd[1219]: time="2025-04-18T13:15:08Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-p7rrs_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4ebc1ed1916d754a9634a8f1750d5497015cdddc13098d951cde21de4a48f289\""
Apr 18 13:15:08 minikube cri-dockerd[1219]: time="2025-04-18T13:15:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/874a53b77b0411dc48d29c74eded662c8c57350866a41a4d964f7def782c6571/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 18 13:15:08 minikube cri-dockerd[1219]: time="2025-04-18T13:15:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/72c6a83e664263435767ea58012e8176e1fe9a5db038b53590a6a4cd74ebadd3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 18 13:15:08 minikube cri-dockerd[1219]: time="2025-04-18T13:15:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/758be120a29a8e004ff6a9ee7cec1fa6e50acc802f1bbcc10170db73507642e1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 18 13:15:08 minikube cri-dockerd[1219]: time="2025-04-18T13:15:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e050eed63d8e7122e16c9add12a39ab5f070d32d9e800f88e2cb71f5c1b93cdb/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 18 13:15:09 minikube cri-dockerd[1219]: time="2025-04-18T13:15:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-8694d4445c-9pwp4_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"942965bbe7a16634d1e0702efeb6c2e30f59395768e19003462e0a292e391fa5\""
Apr 18 13:15:18 minikube cri-dockerd[1219]: time="2025-04-18T13:15:18Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Apr 18 13:15:22 minikube cri-dockerd[1219]: time="2025-04-18T13:15:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3a49f976615a4a7b4212fc09a5860dcb815d7c3a66ac274f29d86081428dfdeb/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 18 13:15:23 minikube cri-dockerd[1219]: time="2025-04-18T13:15:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/727748d1b5b465bb8ff8a0b6683bed64001341130d16e86d4b2a2ddeaa0d92d0/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 18 13:15:24 minikube cri-dockerd[1219]: time="2025-04-18T13:15:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4e85fccd0be4f39fefdbe07736fdcf22a7d6ade9246c590d7daab763551d4ce6/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 18 13:15:24 minikube cri-dockerd[1219]: time="2025-04-18T13:15:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fdef3ef99a3d10b730d51cdae746a4198b8577868f1c296fbeb64c7f8531b796/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 18 13:15:24 minikube cri-dockerd[1219]: time="2025-04-18T13:15:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/62cd8e023aae9295f59a67cf752840b8473555961da33eff79d62a1779664cf1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 18 13:15:24 minikube cri-dockerd[1219]: time="2025-04-18T13:15:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6ee1e9a67ff8aa2e12f06cdd7f2db3dc14db8d8182fa879bb3c3937423819b11/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 18 13:15:24 minikube cri-dockerd[1219]: time="2025-04-18T13:15:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3a4a8ebc26d931da66300baa147c7fb21b1c7f76ff8fa2a90777405c87d2e65d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 18 13:15:37 minikube dockerd[965]: time="2025-04-18T13:15:37.822295944Z" level=info msg="ignoring event" container=56033ca650282c7c75d4cb486ee92bf0025c9954df3ed5036532df3c0b55de0f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 18 13:16:37 minikube dockerd[965]: time="2025-04-18T13:16:37.872749847Z" level=info msg="ignoring event" container=ac8f1a933c833b6885a4146ed31ad49506df371d0d018bea7359a11490f5681d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 18 13:17:46 minikube dockerd[965]: time="2025-04-18T13:17:46.553714775Z" level=info msg="ignoring event" container=ae3979341d2333c867d4e690f86e603e459a48de5389a94d0dc86234ce243884 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 18 13:20:05 minikube dockerd[965]: time="2025-04-18T13:20:05.113912373Z" level=info msg="ignoring event" container=8ae1ddb654c0e72d80fcf16106878b41496fdffaa224e24b9c67af19d5aa2d1a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 18 13:21:53 minikube dockerd[965]: time="2025-04-18T13:21:53.974085790Z" level=info msg="ignoring event" container=3699039919cb900ec8b441072aeac558c3001468d89a9475701cb3b220e007ae module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 18 13:28:20 minikube dockerd[965]: time="2025-04-18T13:28:20.851359072Z" level=info msg="ignoring event" container=f1b9e6fc055bd84510d1615352f3648738e99a8708d6952858df52289f477538 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 18 13:31:37 minikube dockerd[965]: time="2025-04-18T13:31:37.716835834Z" level=info msg="ignoring event" container=49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
49853b7570c73       f9e6633099356                                                                                          4 minutes ago       Exited              catalog-service             13                  62cd8e023aae9       catalog-service-6f5d4b4f76-966d9
a4fa4909329e0       6e38f40d628db                                                                                          20 minutes ago      Running             storage-provisioner         11                  727748d1b5b46       storage-provisioner
5b77506966c44       115053965e86b                                                                                          20 minutes ago      Running             dashboard-metrics-scraper   4                   6ee1e9a67ff8a       dashboard-metrics-scraper-7fd5cb4ddc-p7rrs
ce05cbb18b793       dbceeef68585d                                                                                          20 minutes ago      Running             polar-postgres              1                   3a4a8ebc26d93       polar-postgres-5c854695c8-jwfdh
82f418e16d436       ead0a4a53df89                                                                                          20 minutes ago      Running             coredns                     6                   fdef3ef99a3d1       coredns-5dd5756b68-gc7x7
353c7589ee66e       07655ddf2eebe                                                                                          20 minutes ago      Running             kubernetes-dashboard        6                   4e85fccd0be4f       kubernetes-dashboard-8694d4445c-9pwp4
56033ca650282       6e38f40d628db                                                                                          20 minutes ago      Exited              storage-provisioner         10                  727748d1b5b46       storage-provisioner
81ea107af513d       bfc896cf80fba                                                                                          20 minutes ago      Running             kube-proxy                  6                   3a49f976615a4       kube-proxy-zvrvl
b3c7ab0f119b9       5374347291230                                                                                          20 minutes ago      Running             kube-apiserver              7                   e050eed63d8e7       kube-apiserver-minikube
3fa9af95179e7       73deb9a3f7025                                                                                          20 minutes ago      Running             etcd                        7                   874a53b77b041       etcd-minikube
76b40d46196eb       10baa1ca17068                                                                                          20 minutes ago      Running             kube-controller-manager     7                   758be120a29a8       kube-controller-manager-minikube
522f214ebaae0       6d1b4fd1b182d                                                                                          20 minutes ago      Running             kube-scheduler              7                   72c6a83e66426       kube-scheduler-minikube
baf8e8d5e2fb9       postgres@sha256:f055b09e632d40c562d80e1078c21362d720d3c8c040f65edf6cb609229f09d3                       About an hour ago   Exited              polar-postgres              0                   04da9d95f3fd3       polar-postgres-5c854695c8-jwfdh
43c9605dd8d25       ead0a4a53df89                                                                                          2 hours ago         Exited              coredns                     5                   9ea059f6fa8fc       coredns-5dd5756b68-gc7x7
809cd7c50c659       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   2 hours ago         Exited              dashboard-metrics-scraper   3                   4ebc1ed1916d7       dashboard-metrics-scraper-7fd5cb4ddc-p7rrs
b2f3ba3aa82d2       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93         2 hours ago         Exited              kubernetes-dashboard        5                   942965bbe7a16       kubernetes-dashboard-8694d4445c-9pwp4
7ffcec4c346e9       bfc896cf80fba                                                                                          2 hours ago         Exited              kube-proxy                  5                   f0b4c228657c8       kube-proxy-zvrvl
f4aad0be47e9d       6d1b4fd1b182d                                                                                          2 hours ago         Exited              kube-scheduler              6                   84abb21316ff3       kube-scheduler-minikube
30f7522c2548f       73deb9a3f7025                                                                                          2 hours ago         Exited              etcd                        6                   e7b2d4216a140       etcd-minikube
327cc35e80df6       10baa1ca17068                                                                                          2 hours ago         Exited              kube-controller-manager     6                   e40a24ea2310e       kube-controller-manager-minikube
62da2bb5fb6fb       5374347291230                                                                                          2 hours ago         Exited              kube-apiserver              6                   97700cbbb8f68       kube-apiserver-minikube

* 
* ==> coredns [43c9605dd8d2] <==
* [ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:50453->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:40301 - 22003 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.101153319s
[INFO] 10.244.0.20:46167 - 594 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000431871s
[INFO] 10.244.0.20:33809 - 45413 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000258812s
[INFO] 10.244.0.20:40556 - 25155 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000222794s
[INFO] 10.244.0.20:42331 - 59499 "A IN config-service. udp 32 false 512" - - 0 2.000733297s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:51028->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:49891 - 59499 "A IN config-service. udp 32 false 512" - - 0 2.000636198s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:43161->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:42041 - 5027 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.000920748s
[INFO] 10.244.0.20:37437 - 12918 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00074272s
[INFO] 10.244.0.20:34927 - 59061 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000395549s
[INFO] 10.244.0.20:37759 - 45607 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000220648s
[INFO] 10.244.0.20:48107 - 50928 "A IN config-service. udp 32 false 512" - - 0 2.000622424s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:46719->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:57832 - 50928 "A IN config-service. udp 32 false 512" - - 0 2.001505335s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:57287->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:46153 - 16808 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.001347267s
[INFO] 10.244.0.20:50557 - 32217 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000651304s
[INFO] 10.244.0.20:44967 - 26124 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000721866s
[INFO] 10.244.0.20:45305 - 54726 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000427528s
[INFO] 10.244.0.20:40275 - 45123 "A IN config-service. udp 32 false 512" - - 0 2.002353723s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:58181->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:59673 - 45123 "A IN config-service. udp 32 false 512" - - 0 2.002910762s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:60287->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:45496 - 41298 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.000496563s
[INFO] 10.244.0.20:50003 - 49168 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000655711s
[INFO] 10.244.0.20:48219 - 9432 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000337027s
[INFO] 10.244.0.20:51237 - 17214 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000235618s
[INFO] 10.244.0.20:47112 - 47923 "A IN config-service. udp 32 false 512" - - 0 2.001470854s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:41413->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:33969 - 47923 "A IN config-service. udp 32 false 512" - - 0 2.001247342s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:38753->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:59439 - 509 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.001874337s
[INFO] 10.244.0.20:43867 - 13448 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000372083s
[INFO] 10.244.0.20:35531 - 24743 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000324651s
[INFO] 10.244.0.20:50008 - 29026 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000278485s
[INFO] 10.244.0.20:49605 - 38873 "A IN config-service. udp 32 false 512" - - 0 2.001170323s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:57503->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:48717 - 38873 "A IN config-service. udp 32 false 512" - - 0 2.001734072s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:54312->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:43474 - 52718 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.000453204s
[INFO] 10.244.0.20:58246 - 47388 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00064446s
[INFO] 10.244.0.20:45710 - 17450 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000371643s
[INFO] 10.244.0.20:53901 - 64319 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000306142s
[INFO] 10.244.0.20:36064 - 15257 "A IN config-service. udp 32 false 512" - - 0 2.001049228s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:46909->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:52758 - 15257 "A IN config-service. udp 32 false 512" - - 0 2.001180169s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:42795->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:34926 - 35209 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.000469908s
[INFO] 10.244.0.20:55245 - 46779 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00048368s
[INFO] 10.244.0.20:37246 - 13391 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000448366s
[INFO] 10.244.0.20:58734 - 24014 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000256931s
[INFO] 10.244.0.20:58582 - 16877 "A IN config-service. udp 32 false 512" - - 0 2.000920848s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:34742->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:56188 - 16877 "A IN config-service. udp 32 false 512" - - 0 2.000418813s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.18:48402->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.20:46725 - 51641 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.081966708s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [82f418e16d43] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:47240 - 57130 "HINFO IN 2559039386730873623.6513118927655755952. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.285321138s
[INFO] 10.244.0.22:54129 - 4284 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000689349s
[INFO] 10.244.0.22:36993 - 53472 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000314457s
[INFO] 10.244.0.22:39361 - 63222 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000209194s
[INFO] 10.244.0.22:51852 - 32647 "A IN config-service. udp 32 false 512" - - 0 6.004473793s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:56239->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:51852 - 32647 "A IN config-service. udp 32 false 512" - - 0 4.056661402s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:40465->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:55763 - 4116 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.000291107s
[INFO] 10.244.0.22:51569 - 24311 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000807877s
[INFO] 10.244.0.22:46322 - 5761 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000309238s
[INFO] 10.244.0.22:45236 - 31777 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000316554s
[INFO] 10.244.0.22:34260 - 22147 "A IN config-service. udp 32 false 512" - - 0 2.000680513s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:54432->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:53836 - 22147 "A IN config-service. udp 32 false 512" - - 0 2.000931632s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:60728->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:49134 - 2182 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.000376886s
[INFO] 10.244.0.22:52810 - 14359 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000412755s
[INFO] 10.244.0.22:42099 - 13815 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000242874s
[INFO] 10.244.0.22:42709 - 44577 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.00023899s
[INFO] 10.244.0.22:54829 - 16787 "A IN config-service. udp 32 false 512" - - 0 6.002843552s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:54038->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:54829 - 16787 "A IN config-service. udp 32 false 512" - - 0 4.007677983s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:33365->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:47918 - 9540 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.006495032s
[INFO] 10.244.0.22:45298 - 11721 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000560428s
[INFO] 10.244.0.22:47114 - 16593 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000593295s
[INFO] 10.244.0.22:34332 - 53515 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000475434s
[INFO] 10.244.0.22:43110 - 47766 "A IN config-service. udp 32 false 512" - - 0 2.001518632s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:33562->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:51583 - 47766 "A IN config-service. udp 32 false 512" - - 0 2.000967385s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:59907->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:34513 - 25287 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.001729032s
[INFO] 10.244.0.22:39368 - 5980 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000674051s
[INFO] 10.244.0.22:45462 - 12051 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000419681s
[INFO] 10.244.0.22:35233 - 58945 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000411981s
[INFO] 10.244.0.22:42865 - 1434 "A IN config-service. udp 32 false 512" - - 0 6.003796487s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:33061->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:42865 - 1434 "A IN config-service. udp 32 false 512" - - 0 4.00447049s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:57704->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:47442 - 64521 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.00036564s
[INFO] 10.244.0.22:45457 - 52449 "A IN config-service.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000236743s
[INFO] 10.244.0.22:46450 - 21593 "A IN config-service.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000171045s
[INFO] 10.244.0.22:33761 - 59151 "A IN config-service.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000133219s
[INFO] 10.244.0.22:37964 - 42354 "A IN config-service. udp 32 false 512" - - 0 2.001326362s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:52002->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:39087 - 42354 "A IN config-service. udp 32 false 512" - - 0 2.001128724s
[ERROR] plugin/errors: 2 config-service. A: read udp 10.244.0.21:59178->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.22:45971 - 47840 "A IN polar-postgres.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.000613531s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_04_29T17_02_27_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 29 Apr 2024 13:02:22 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 18 Apr 2025 13:35:53 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 18 Apr 2025 13:31:04 +0000   Mon, 29 Apr 2024 13:02:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 18 Apr 2025 13:31:04 +0000   Mon, 29 Apr 2024 13:02:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 18 Apr 2025 13:31:04 +0000   Mon, 29 Apr 2024 13:02:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 18 Apr 2025 13:31:04 +0000   Mon, 29 Apr 2024 13:02:27 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7999912Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7999912Ki
  pods:               110
System Info:
  Machine ID:                 afa6e7d48a844c97bfa527e0a37c6d9a
  System UUID:                afa6e7d48a844c97bfa527e0a37c6d9a
  Boot ID:                    e86a89f8-1a7f-43e7-9d07-d13da0b17cac
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     catalog-service-6f5d4b4f76-966d9              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         53m
  default                     polar-postgres-5c854695c8-jwfdh               100m (0%!)(MISSING)     200m (1%!)(MISSING)   60Mi (0%!)(MISSING)        120Mi (1%!)(MISSING)     72m
  kube-system                 coredns-5dd5756b68-gc7x7                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     354d
  kube-system                 etcd-minikube                                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         354d
  kube-system                 kube-apiserver-minikube                       250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         354d
  kube-system                 kube-controller-manager-minikube              200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         354d
  kube-system                 kube-proxy-zvrvl                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         354d
  kube-system                 kube-scheduler-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         354d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         354d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-p7rrs    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-9pwp4         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (5%!)(MISSING)   200m (1%!)(MISSING)
  memory             230Mi (2%!)(MISSING)  290Mi (3%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 20m                kube-proxy       
  Normal  Starting                 20m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  20m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  20m (x8 over 20m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    20m (x8 over 20m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     20m (x7 over 20m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           20m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Apr18 11:36] PCI: Fatal: No config space access function found
[  +0.021515] PCI: System does not support PCI
[  +0.271375] kvm: already loaded the other module
[  +1.096141] FS-Cache: Duplicate cookie detected
[  +0.000673] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000633] FS-Cache: O-cookie d=000000008b7af303{9P.session} n=00000000d7c782f3
[  +0.000654] FS-Cache: O-key=[10] '34323934393337343339'
[  +0.000834] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000784] FS-Cache: N-cookie d=000000008b7af303{9P.session} n=000000000b0f588f
[  +0.000591] FS-Cache: N-key=[10] '34323934393337343339'
[  +0.864579] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000008]  failed 2
[  +0.015746] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Yerevan not found. Is the tzdata package installed?
[  +0.779982] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001261] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000934] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001394] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.458660] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001918] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.002553] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.003471] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000009]  failed 2
[  +0.008260] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.002003] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.029266] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Yerevan not found. Is the tzdata package installed?
[  +0.136194] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001494] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001628] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001620] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.602386] netlink: 'init': attribute type 4 has an invalid length.
[  +0.460719] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.
[Apr18 12:23] CPU: 10 PID: 299 Comm: weston Not tainted 5.15.146.1-microsoft-standard-WSL2 #1
[  +0.000131] RIP: 0033:0x7f9c78203e6c
[  +0.000356] Code: ff ff 0f 46 ea eb 99 0f 1f 80 00 00 00 00 b8 ba 00 00 00 0f 05 89 c5 e8 32 d5 04 00 44 89 e2 89 ee 89 c7 b8 ea 00 00 00 0f 05 <89> c5 f7 dd 3d 00 f0 ff ff b8 00 00 00 00 0f 47 c5 48 83 ec 80 5b
[  +0.000003] RSP: 002b:00007ffcdc16e860 EFLAGS: 00000246 ORIG_RAX: 00000000000000ea
[  +0.000007] RAX: 0000000000000000 RBX: 00007f9c74bc4ec0 RCX: 00007f9c78203e6c
[  +0.000002] RDX: 0000000000000006 RSI: 000000000000000a RDI: 000000000000000a
[  +0.000002] RBP: 000000000000000a R08: 00007ffcdc16e928 R09: 0000000000000000
[  +0.000001] R10: 0000000000000008 R11: 0000000000000246 R12: 0000000000000006
[  +0.000002] R13: 00007ffcdc16ea80 R14: 0000000000000010 R15: 00007f9c707a7000
[  +0.000002] FS:  00007f9c74bc4ec0 GS:  0000000000000000
[  +0.028332] CPU: 6 PID: 236 Comm: weston Not tainted 5.15.146.1-microsoft-standard-WSL2 #1
[  +0.000007] RIP: 0033:0x7fdad68cde6c
[  +0.000008] Code: ff ff 0f 46 ea eb 99 0f 1f 80 00 00 00 00 b8 ba 00 00 00 0f 05 89 c5 e8 32 d5 04 00 44 89 e2 89 ee 89 c7 b8 ea 00 00 00 0f 05 <89> c5 f7 dd 3d 00 f0 ff ff b8 00 00 00 00 0f 47 c5 48 83 ec 80 5b
[  +0.000003] RSP: 002b:00007ffeb58eb670 EFLAGS: 00000246 ORIG_RAX: 00000000000000ea
[  +0.000006] RAX: 0000000000000000 RBX: 00007fdad328eec0 RCX: 00007fdad68cde6c
[  +0.000002] RDX: 0000000000000006 RSI: 000000000000000e RDI: 000000000000000e
[  +0.000002] RBP: 000000000000000e R08: 00007ffeb58eb738 R09: 0000000000000000
[  +0.000001] R10: 0000000000000008 R11: 0000000000000246 R12: 0000000000000006
[  +0.000002] R13: 00007ffeb58eb890 R14: 0000000000000010 R15: 00007fdad0674000
[  +0.000003] FS:  00007fdad328eec0 GS:  0000000000000000

* 
* ==> etcd [30f7522c2548] <==
* {"level":"warn","ts":"2025-04-18T13:09:19.333674Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"283.519692ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/jobs/default/\" range_end:\"/registry/jobs/default0\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-04-18T13:09:19.333713Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"283.025909ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/\" range_end:\"/registry/pods/default0\" ","response":"range_response_count:2 size:6651"}
{"level":"warn","ts":"2025-04-18T13:09:19.333744Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"283.787381ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/cronjobs/default/\" range_end:\"/registry/cronjobs/default0\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-18T13:09:19.333781Z","caller":"traceutil/trace.go:171","msg":"trace[589817995] range","detail":"{range_begin:/registry/jobs/default/; range_end:/registry/jobs/default0; response_count:0; response_revision:10713; }","duration":"283.637237ms","start":"2025-04-18T13:09:19.050114Z","end":"2025-04-18T13:09:19.333751Z","steps":["trace[589817995] 'agreement among raft nodes before linearized reading'  (duration: 183.795383ms)","trace[589817995] 'range keys from in-memory index tree'  (duration: 99.699363ms)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:09:19.333798Z","caller":"traceutil/trace.go:171","msg":"trace[1473317121] range","detail":"{range_begin:/registry/cronjobs/default/; range_end:/registry/cronjobs/default0; response_count:0; response_revision:10713; }","duration":"283.828571ms","start":"2025-04-18T13:09:19.04995Z","end":"2025-04-18T13:09:19.333778Z","steps":["trace[1473317121] 'agreement among raft nodes before linearized reading'  (duration: 183.970915ms)","trace[1473317121] 'range keys from in-memory index tree'  (duration: 99.806366ms)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:09:19.333847Z","caller":"traceutil/trace.go:171","msg":"trace[1215081530] range","detail":"{range_begin:/registry/pods/default/; range_end:/registry/pods/default0; response_count:2; response_revision:10713; }","duration":"283.150184ms","start":"2025-04-18T13:09:19.050664Z","end":"2025-04-18T13:09:19.333815Z","steps":["trace[1215081530] 'agreement among raft nodes before linearized reading'  (duration: 183.197316ms)","trace[1215081530] 'range keys from in-memory index tree'  (duration: 99.786226ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:09:19.333866Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"295.199096ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/\" range_end:\"/registry/events/default0\" ","response":"range_response_count:16 size:12243"}
{"level":"info","ts":"2025-04-18T13:09:19.333921Z","caller":"traceutil/trace.go:171","msg":"trace[1636872047] range","detail":"{range_begin:/registry/events/default/; range_end:/registry/events/default0; response_count:16; response_revision:10713; }","duration":"295.255899ms","start":"2025-04-18T13:09:19.038646Z","end":"2025-04-18T13:09:19.333902Z","steps":["trace[1636872047] 'agreement among raft nodes before linearized reading'  (duration: 195.156901ms)","trace[1636872047] 'range keys from in-memory index tree'  (duration: 99.947623ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:09:19.333939Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"284.856008ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/\" range_end:\"/registry/events/default0\" ","response":"range_response_count:16 size:12243"}
{"level":"info","ts":"2025-04-18T13:09:19.334075Z","caller":"traceutil/trace.go:171","msg":"trace[36623338] range","detail":"{range_begin:/registry/events/default/; range_end:/registry/events/default0; response_count:16; response_revision:10713; }","duration":"284.996652ms","start":"2025-04-18T13:09:19.049059Z","end":"2025-04-18T13:09:19.334055Z","steps":["trace[36623338] 'agreement among raft nodes before linearized reading'  (duration: 184.891715ms)","trace[36623338] 'range keys from in-memory index tree'  (duration: 99.850291ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:09:19.334153Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"284.021734ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/\" range_end:\"/registry/replicasets/default0\" ","response":"range_response_count:2 size:4742"}
{"level":"warn","ts":"2025-04-18T13:09:19.334229Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"199.391571ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:868"}
{"level":"info","ts":"2025-04-18T13:09:19.334247Z","caller":"traceutil/trace.go:171","msg":"trace[684474481] range","detail":"{range_begin:/registry/replicasets/default/; range_end:/registry/replicasets/default0; response_count:2; response_revision:10713; }","duration":"284.121475ms","start":"2025-04-18T13:09:19.050111Z","end":"2025-04-18T13:09:19.334233Z","steps":["trace[684474481] 'agreement among raft nodes before linearized reading'  (duration: 183.789425ms)","trace[684474481] 'range keys from in-memory index tree'  (duration: 100.199499ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:09:19.334262Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"283.643289ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/\" range_end:\"/registry/pods/default0\" ","response":"range_response_count:2 size:6651"}
{"level":"info","ts":"2025-04-18T13:09:19.334278Z","caller":"traceutil/trace.go:171","msg":"trace[1818220274] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:10713; }","duration":"199.44527ms","start":"2025-04-18T13:09:19.134817Z","end":"2025-04-18T13:09:19.334262Z","steps":["trace[1818220274] 'agreement among raft nodes before linearized reading'  (duration: 99.035003ms)","trace[1818220274] 'range keys from in-memory index tree'  (duration: 100.322902ms)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:09:19.334301Z","caller":"traceutil/trace.go:171","msg":"trace[837488810] range","detail":"{range_begin:/registry/pods/default/; range_end:/registry/pods/default0; response_count:2; response_revision:10713; }","duration":"283.681793ms","start":"2025-04-18T13:09:19.050607Z","end":"2025-04-18T13:09:19.334289Z","steps":["trace[837488810] 'agreement among raft nodes before linearized reading'  (duration: 183.278156ms)","trace[837488810] 'range keys from in-memory index tree'  (duration: 100.333615ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:09:19.334351Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"285.087005ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/default/\" range_end:\"/registry/daemonsets/default0\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-04-18T13:09:19.33367Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"283.906554ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/\" range_end:\"/registry/deployments/default0\" ","response":"range_response_count:2 size:6823"}
{"level":"info","ts":"2025-04-18T13:09:19.334409Z","caller":"traceutil/trace.go:171","msg":"trace[1806166942] range","detail":"{range_begin:/registry/deployments/default/; range_end:/registry/deployments/default0; response_count:2; response_revision:10713; }","duration":"284.692351ms","start":"2025-04-18T13:09:19.049708Z","end":"2025-04-18T13:09:19.3344Z","steps":["trace[1806166942] 'agreement among raft nodes before linearized reading'  (duration: 184.222439ms)","trace[1806166942] 'range keys from in-memory index tree'  (duration: 99.634064ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:09:19.334439Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"283.794714ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/\" range_end:\"/registry/pods/default0\" ","response":"range_response_count:2 size:6651"}
{"level":"warn","ts":"2025-04-18T13:09:19.334442Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"283.895996ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/\" range_end:\"/registry/pods/default0\" ","response":"range_response_count:2 size:6651"}
{"level":"info","ts":"2025-04-18T13:09:19.334468Z","caller":"traceutil/trace.go:171","msg":"trace[819251882] range","detail":"{range_begin:/registry/pods/default/; range_end:/registry/pods/default0; response_count:2; response_revision:10713; }","duration":"283.819988ms","start":"2025-04-18T13:09:19.050638Z","end":"2025-04-18T13:09:19.334458Z","steps":["trace[819251882] 'agreement among raft nodes before linearized reading'  (duration: 183.239566ms)","trace[819251882] 'range keys from in-memory index tree'  (duration: 100.536952ms)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:09:19.334484Z","caller":"traceutil/trace.go:171","msg":"trace[346372290] range","detail":"{range_begin:/registry/pods/default/; range_end:/registry/pods/default0; response_count:2; response_revision:10713; }","duration":"283.934197ms","start":"2025-04-18T13:09:19.050536Z","end":"2025-04-18T13:09:19.33447Z","steps":["trace[346372290] 'agreement among raft nodes before linearized reading'  (duration: 183.356689ms)","trace[346372290] 'range keys from in-memory index tree'  (duration: 100.507781ms)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:09:19.334414Z","caller":"traceutil/trace.go:171","msg":"trace[794483530] range","detail":"{range_begin:/registry/daemonsets/default/; range_end:/registry/daemonsets/default0; response_count:0; response_revision:10713; }","duration":"285.139191ms","start":"2025-04-18T13:09:19.049244Z","end":"2025-04-18T13:09:19.334383Z","steps":["trace[794483530] 'agreement among raft nodes before linearized reading'  (duration: 184.693958ms)","trace[794483530] 'range keys from in-memory index tree'  (duration: 100.374023ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:09:19.334365Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"296.140372ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/\" range_end:\"/registry/services/specs/default0\" ","response":"range_response_count:2 size:1766"}
{"level":"info","ts":"2025-04-18T13:09:19.334644Z","caller":"traceutil/trace.go:171","msg":"trace[351653568] range","detail":"{range_begin:/registry/services/specs/default/; range_end:/registry/services/specs/default0; response_count:2; response_revision:10713; }","duration":"296.429381ms","start":"2025-04-18T13:09:19.038203Z","end":"2025-04-18T13:09:19.334632Z","steps":["trace[351653568] 'agreement among raft nodes before linearized reading'  (duration: 195.558295ms)","trace[351653568] 'range keys from in-memory index tree'  (duration: 99.620841ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:09:19.334831Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"285.781018ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/\" range_end:\"/registry/events/default0\" ","response":"range_response_count:16 size:12243"}
{"level":"info","ts":"2025-04-18T13:09:19.334888Z","caller":"traceutil/trace.go:171","msg":"trace[286032019] range","detail":"{range_begin:/registry/events/default/; range_end:/registry/events/default0; response_count:16; response_revision:10713; }","duration":"285.847634ms","start":"2025-04-18T13:09:19.049026Z","end":"2025-04-18T13:09:19.334873Z","steps":["trace[286032019] 'agreement among raft nodes before linearized reading'  (duration: 184.932402ms)","trace[286032019] 'range keys from in-memory index tree'  (duration: 100.59397ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:09:19.336665Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"287.695054ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/\" range_end:\"/registry/events/default0\" ","response":"range_response_count:16 size:12243"}
{"level":"info","ts":"2025-04-18T13:09:19.336776Z","caller":"traceutil/trace.go:171","msg":"trace[402659175] range","detail":"{range_begin:/registry/events/default/; range_end:/registry/events/default0; response_count:16; response_revision:10713; }","duration":"287.81868ms","start":"2025-04-18T13:09:19.048929Z","end":"2025-04-18T13:09:19.336748Z","steps":["trace[402659175] 'agreement among raft nodes before linearized reading'  (duration: 185.036383ms)","trace[402659175] 'range keys from in-memory index tree'  (duration: 99.402276ms)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:10:11.933478Z","caller":"traceutil/trace.go:171","msg":"trace[1614934784] transaction","detail":"{read_only:false; response_revision:10755; number_of_response:1; }","duration":"101.217547ms","start":"2025-04-18T13:10:11.832241Z","end":"2025-04-18T13:10:11.933459Z","steps":["trace[1614934784] 'process raft request'  (duration: 101.123894ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:11:05.228971Z","caller":"traceutil/trace.go:171","msg":"trace[975964455] transaction","detail":"{read_only:false; response_revision:10800; number_of_response:1; }","duration":"195.19365ms","start":"2025-04-18T13:11:05.033746Z","end":"2025-04-18T13:11:05.228939Z","steps":["trace[975964455] 'process raft request'  (duration: 194.935624ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:11:28.424952Z","caller":"traceutil/trace.go:171","msg":"trace[589829200] transaction","detail":"{read_only:false; response_revision:10818; number_of_response:1; }","duration":"204.572274ms","start":"2025-04-18T13:11:28.220356Z","end":"2025-04-18T13:11:28.424929Z","steps":["trace[589829200] 'process raft request'  (duration: 204.436138ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:11:28.627686Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.236632ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-18T13:11:28.627832Z","caller":"traceutil/trace.go:171","msg":"trace[1675268962] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:10818; }","duration":"105.402554ms","start":"2025-04-18T13:11:28.52241Z","end":"2025-04-18T13:11:28.627813Z","steps":["trace[1675268962] 'range keys from in-memory index tree'  (duration: 105.104448ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:11:32.637725Z","caller":"traceutil/trace.go:171","msg":"trace[1312878527] linearizableReadLoop","detail":"{readStateIndex:13385; appliedIndex:13384; }","duration":"106.769293ms","start":"2025-04-18T13:11:32.530885Z","end":"2025-04-18T13:11:32.637654Z","steps":["trace[1312878527] 'read index received'  (duration: 8.151015ms)","trace[1312878527] 'applied index is now lower than readState.Index'  (duration: 98.615605ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:11:32.638051Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.1874ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-04-18T13:11:32.638102Z","caller":"traceutil/trace.go:171","msg":"trace[1560907953] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:10820; }","duration":"107.257194ms","start":"2025-04-18T13:11:32.530826Z","end":"2025-04-18T13:11:32.638084Z","steps":["trace[1560907953] 'agreement among raft nodes before linearized reading'  (duration: 107.015923ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:11:37.338227Z","caller":"traceutil/trace.go:171","msg":"trace[1000465816] transaction","detail":"{read_only:false; response_revision:10825; number_of_response:1; }","duration":"105.727985ms","start":"2025-04-18T13:11:37.232457Z","end":"2025-04-18T13:11:37.338185Z","steps":["trace[1000465816] 'process raft request'  (duration: 105.446923ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:11:37.720372Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"192.202283ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-18T13:11:37.720508Z","caller":"traceutil/trace.go:171","msg":"trace[199685725] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:10825; }","duration":"192.355343ms","start":"2025-04-18T13:11:37.528118Z","end":"2025-04-18T13:11:37.720473Z","steps":["trace[199685725] 'range keys from in-memory index tree'  (duration: 192.041978ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:11:39.23524Z","caller":"traceutil/trace.go:171","msg":"trace[551793638] transaction","detail":"{read_only:false; response_revision:10826; number_of_response:1; }","duration":"107.093724ms","start":"2025-04-18T13:11:39.128114Z","end":"2025-04-18T13:11:39.235207Z","steps":["trace[551793638] 'process raft request'  (duration: 106.824769ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:11:41.720174Z","caller":"traceutil/trace.go:171","msg":"trace[345499023] transaction","detail":"{read_only:false; response_revision:10827; number_of_response:1; }","duration":"190.605568ms","start":"2025-04-18T13:11:41.529535Z","end":"2025-04-18T13:11:41.72014Z","steps":["trace[345499023] 'process raft request'  (duration: 190.431511ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:11:42.934463Z","caller":"traceutil/trace.go:171","msg":"trace[107780920] transaction","detail":"{read_only:false; response_revision:10828; number_of_response:1; }","duration":"107.314972ms","start":"2025-04-18T13:11:42.827084Z","end":"2025-04-18T13:11:42.934399Z","steps":["trace[107780920] 'process raft request'  (duration: 94.312029ms)","trace[107780920] 'marshal mvccpb.KeyValue' {req_type:put; key:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; req_size:670; } (duration: 12.737511ms)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:12:34.832504Z","caller":"traceutil/trace.go:171","msg":"trace[601025428] transaction","detail":"{read_only:false; response_revision:10874; number_of_response:1; }","duration":"106.353721ms","start":"2025-04-18T13:12:34.726094Z","end":"2025-04-18T13:12:34.832448Z","steps":["trace[601025428] 'process raft request'  (duration: 103.301826ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:12:45.118051Z","caller":"traceutil/trace.go:171","msg":"trace[392945924] transaction","detail":"{read_only:false; response_revision:10882; number_of_response:1; }","duration":"108.380174ms","start":"2025-04-18T13:12:45.009646Z","end":"2025-04-18T13:12:45.118026Z","steps":["trace[392945924] 'process raft request'  (duration: 108.312598ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:12:45.1185Z","caller":"traceutil/trace.go:171","msg":"trace[1769294130] transaction","detail":"{read_only:false; response_revision:10881; number_of_response:1; }","duration":"114.646221ms","start":"2025-04-18T13:12:45.003841Z","end":"2025-04-18T13:12:45.118487Z","steps":["trace[1769294130] 'process raft request'  (duration: 113.941649ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:13:42.360048Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10683}
{"level":"info","ts":"2025-04-18T13:13:42.361267Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":10683,"took":"1.024972ms","hash":72818391}
{"level":"info","ts":"2025-04-18T13:13:42.361327Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":72818391,"revision":10683,"compact-revision":10437}
{"level":"info","ts":"2025-04-18T13:14:10.820491Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-04-18T13:14:10.824329Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-04-18T13:14:10.923327Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-18T13:14:10.924817Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-18T13:14:11.717227Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-18T13:14:11.717375Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-04-18T13:14:11.720858Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-04-18T13:14:11.930359Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-18T13:14:11.931545Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-18T13:14:11.931615Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [3fa9af95179e] <==
* {"level":"info","ts":"2025-04-18T13:19:54.413564Z","caller":"traceutil/trace.go:171","msg":"trace[1301026495] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:11253; }","duration":"106.143918ms","start":"2025-04-18T13:19:54.307375Z","end":"2025-04-18T13:19:54.413519Z","steps":["trace[1301026495] 'agreement among raft nodes before linearized reading'  (duration: 105.737408ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:19:57.011478Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"194.652089ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-18T13:19:57.011626Z","caller":"traceutil/trace.go:171","msg":"trace[1052017974] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11255; }","duration":"194.814019ms","start":"2025-04-18T13:19:56.816791Z","end":"2025-04-18T13:19:57.011605Z","steps":["trace[1052017974] 'range keys from in-memory index tree'  (duration: 194.520937ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:19:59.120408Z","caller":"traceutil/trace.go:171","msg":"trace[357882950] linearizableReadLoop","detail":"{readStateIndex:13918; appliedIndex:13918; }","duration":"105.768844ms","start":"2025-04-18T13:19:59.014609Z","end":"2025-04-18T13:19:59.120378Z","steps":["trace[357882950] 'read index received'  (duration: 105.753513ms)","trace[357882950] 'applied index is now lower than readState.Index'  (duration: 12.814s)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:19:59.120545Z","caller":"traceutil/trace.go:171","msg":"trace[1122645659] transaction","detail":"{read_only:false; response_revision:11257; number_of_response:1; }","duration":"109.14625ms","start":"2025-04-18T13:19:59.011363Z","end":"2025-04-18T13:19:59.120509Z","steps":["trace[1122645659] 'process raft request'  (duration: 108.585115ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:19:59.120706Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.106337ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-18T13:19:59.12075Z","caller":"traceutil/trace.go:171","msg":"trace[1180562696] range","detail":"{range_begin:/registry/statefulsets/; range_end:/registry/statefulsets0; response_count:0; response_revision:11257; }","duration":"106.176028ms","start":"2025-04-18T13:19:59.014562Z","end":"2025-04-18T13:19:59.120738Z","steps":["trace[1180562696] 'agreement among raft nodes before linearized reading'  (duration: 105.994279ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:21:08.003942Z","caller":"traceutil/trace.go:171","msg":"trace[966097452] transaction","detail":"{read_only:false; response_revision:11323; number_of_response:1; }","duration":"111.112404ms","start":"2025-04-18T13:21:07.892795Z","end":"2025-04-18T13:21:08.003907Z","steps":["trace[966097452] 'process raft request'  (duration: 110.934277ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:21:19.905879Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.085816ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-18T13:21:19.906025Z","caller":"traceutil/trace.go:171","msg":"trace[1095627075] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11334; }","duration":"102.247742ms","start":"2025-04-18T13:21:19.803747Z","end":"2025-04-18T13:21:19.905994Z","steps":["trace[1095627075] 'agreement among raft nodes before linearized reading'  (duration: 85.211736ms)","trace[1095627075] 'range keys from in-memory index tree'  (duration: 16.840466ms)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:21:19.89188Z","caller":"traceutil/trace.go:171","msg":"trace[1274420356] transaction","detail":"{read_only:false; response_revision:11334; number_of_response:1; }","duration":"194.150525ms","start":"2025-04-18T13:21:19.697695Z","end":"2025-04-18T13:21:19.891846Z","steps":["trace[1274420356] 'process raft request'  (duration: 191.050725ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:21:29.099993Z","caller":"traceutil/trace.go:171","msg":"trace[1596798481] transaction","detail":"{read_only:false; response_revision:11341; number_of_response:1; }","duration":"108.066945ms","start":"2025-04-18T13:21:28.991893Z","end":"2025-04-18T13:21:29.09996Z","steps":["trace[1596798481] 'process raft request'  (duration: 107.510049ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:21:30.999408Z","caller":"traceutil/trace.go:171","msg":"trace[1464049375] transaction","detail":"{read_only:false; response_revision:11342; number_of_response:1; }","duration":"209.566213ms","start":"2025-04-18T13:21:30.789814Z","end":"2025-04-18T13:21:30.99938Z","steps":["trace[1464049375] 'process raft request'  (duration: 204.042221ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:21:31.211309Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.458817ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-18T13:21:31.211462Z","caller":"traceutil/trace.go:171","msg":"trace[707455644] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11342; }","duration":"113.611124ms","start":"2025-04-18T13:21:31.097819Z","end":"2025-04-18T13:21:31.21143Z","steps":["trace[707455644] 'range keys from in-memory index tree'  (duration: 113.329679ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:21:33.599527Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.072071ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036667940879143 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:11342 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2025-04-18T13:21:33.599729Z","caller":"traceutil/trace.go:171","msg":"trace[996800715] transaction","detail":"{read_only:false; response_revision:11343; number_of_response:1; }","duration":"294.756517ms","start":"2025-04-18T13:21:33.304946Z","end":"2025-04-18T13:21:33.599702Z","steps":["trace[996800715] 'process raft request'  (duration: 191.262884ms)","trace[996800715] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 102.814212ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:21:34.402539Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"201.492136ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036667940879147 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:11336 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128036667940879145 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-04-18T13:21:34.407697Z","caller":"traceutil/trace.go:171","msg":"trace[1793679520] transaction","detail":"{read_only:false; response_revision:11344; number_of_response:1; }","duration":"208.255632ms","start":"2025-04-18T13:21:34.199414Z","end":"2025-04-18T13:21:34.40767Z","steps":["trace[1793679520] 'compare'  (duration: 200.712689ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:21:34.888916Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"285.339398ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2025-04-18T13:21:34.888995Z","caller":"traceutil/trace.go:171","msg":"trace[1165662774] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:11344; }","duration":"285.433646ms","start":"2025-04-18T13:21:34.603549Z","end":"2025-04-18T13:21:34.888983Z","steps":["trace[1165662774] 'range keys from in-memory index tree'  (duration: 285.177994ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:21:37.093049Z","caller":"traceutil/trace.go:171","msg":"trace[2079158588] transaction","detail":"{read_only:false; response_revision:11345; number_of_response:1; }","duration":"189.242977ms","start":"2025-04-18T13:21:36.903135Z","end":"2025-04-18T13:21:37.092378Z","steps":["trace[2079158588] 'process raft request'  (duration: 187.248872ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:21:39.796938Z","caller":"traceutil/trace.go:171","msg":"trace[1403993117] transaction","detail":"{read_only:false; response_revision:11347; number_of_response:1; }","duration":"394.999527ms","start":"2025-04-18T13:21:39.401755Z","end":"2025-04-18T13:21:39.796754Z","steps":["trace[1403993117] 'process raft request'  (duration: 394.503094ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:21:39.797375Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-18T13:21:39.395969Z","time spent":"401.275118ms","remote":"127.0.0.1:48126","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:11345 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-04-18T13:21:40.098584Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.341544ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036667940879165 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:11341 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2025-04-18T13:21:40.098966Z","caller":"traceutil/trace.go:171","msg":"trace[1583283648] transaction","detail":"{read_only:false; response_revision:11348; number_of_response:1; }","duration":"301.334691ms","start":"2025-04-18T13:21:39.797572Z","end":"2025-04-18T13:21:40.098906Z","steps":["trace[1583283648] 'process raft request'  (duration: 189.460501ms)","trace[1583283648] 'compare'  (duration: 111.105401ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:21:40.099225Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-18T13:21:39.797546Z","time spent":"301.525221ms","remote":"127.0.0.1:48232","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:11341 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2025-04-18T13:21:44.113881Z","caller":"traceutil/trace.go:171","msg":"trace[1692853074] transaction","detail":"{read_only:false; response_revision:11350; number_of_response:1; }","duration":"108.05838ms","start":"2025-04-18T13:21:44.005774Z","end":"2025-04-18T13:21:44.113833Z","steps":["trace[1692853074] 'process raft request'  (duration: 94.411196ms)","trace[1692853074] 'compare'  (duration: 13.338007ms)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:21:47.288442Z","caller":"traceutil/trace.go:171","msg":"trace[794332834] transaction","detail":"{read_only:false; response_revision:11352; number_of_response:1; }","duration":"186.28901ms","start":"2025-04-18T13:21:47.102095Z","end":"2025-04-18T13:21:47.288384Z","steps":["trace[794332834] 'process raft request'  (duration: 103.142113ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:21:50.094868Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"207.524817ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-18T13:21:50.09498Z","caller":"traceutil/trace.go:171","msg":"trace[414448852] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:11352; }","duration":"207.679974ms","start":"2025-04-18T13:21:49.887278Z","end":"2025-04-18T13:21:50.094958Z","steps":["trace[414448852] 'count revisions from in-memory index tree'  (duration: 207.432473ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:21:50.096532Z","caller":"traceutil/trace.go:171","msg":"trace[65722047] transaction","detail":"{read_only:false; response_revision:11353; number_of_response:1; }","duration":"209.060884ms","start":"2025-04-18T13:21:49.887435Z","end":"2025-04-18T13:21:50.096496Z","steps":["trace[65722047] 'process raft request'  (duration: 208.787263ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:21:50.603848Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"216.777311ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036667940879202 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:11352 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2025-04-18T13:21:50.604611Z","caller":"traceutil/trace.go:171","msg":"trace[1434822737] transaction","detail":"{read_only:false; response_revision:11354; number_of_response:1; }","duration":"417.185241ms","start":"2025-04-18T13:21:50.187403Z","end":"2025-04-18T13:21:50.604588Z","steps":["trace[1434822737] 'process raft request'  (duration: 199.573537ms)","trace[1434822737] 'compare'  (duration: 216.574172ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:21:50.604707Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-18T13:21:50.187383Z","time spent":"417.273293ms","remote":"127.0.0.1:48126","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:11352 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-04-18T13:21:50.604834Z","caller":"traceutil/trace.go:171","msg":"trace[66880494] linearizableReadLoop","detail":"{readStateIndex:14040; appliedIndex:14039; }","duration":"216.194974ms","start":"2025-04-18T13:21:50.388629Z","end":"2025-04-18T13:21:50.604823Z","steps":["trace[66880494] 'read index received'  (duration: 40.069s)","trace[66880494] 'applied index is now lower than readState.Index'  (duration: 216.153949ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-18T13:21:50.604888Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"216.288527ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-04-18T13:21:50.604912Z","caller":"traceutil/trace.go:171","msg":"trace[1883911949] range","detail":"{range_begin:/registry/storageclasses/; range_end:/registry/storageclasses0; response_count:0; response_revision:11355; }","duration":"216.314404ms","start":"2025-04-18T13:21:50.38859Z","end":"2025-04-18T13:21:50.604905Z","steps":["trace[1883911949] 'agreement among raft nodes before linearized reading'  (duration: 216.26461ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:21:51.107326Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.608002ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-18T13:21:51.112425Z","caller":"traceutil/trace.go:171","msg":"trace[1367195730] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11355; }","duration":"108.828047ms","start":"2025-04-18T13:21:50.998671Z","end":"2025-04-18T13:21:51.107499Z","steps":["trace[1367195730] 'range keys from in-memory index tree'  (duration: 108.442548ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:28:08.396456Z","caller":"traceutil/trace.go:171","msg":"trace[723399349] transaction","detail":"{read_only:false; response_revision:11436; number_of_response:1; }","duration":"191.694731ms","start":"2025-04-18T13:28:08.20473Z","end":"2025-04-18T13:28:08.396425Z","steps":["trace[723399349] 'process raft request'  (duration: 191.502231ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:28:13.811088Z","caller":"traceutil/trace.go:171","msg":"trace[1445856941] transaction","detail":"{read_only:false; response_revision:11439; number_of_response:1; }","duration":"103.678063ms","start":"2025-04-18T13:28:13.707374Z","end":"2025-04-18T13:28:13.811052Z","steps":["trace[1445856941] 'process raft request'  (duration: 103.136012ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:30:06.406865Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128036667940880039,"retry-timeout":"500ms"}
{"level":"info","ts":"2025-04-18T13:30:06.734535Z","caller":"traceutil/trace.go:171","msg":"trace[2025787401] transaction","detail":"{read_only:false; response_revision:11531; number_of_response:1; }","duration":"890.097982ms","start":"2025-04-18T13:30:05.844399Z","end":"2025-04-18T13:30:06.734497Z","steps":["trace[2025787401] 'process raft request'  (duration: 889.817608ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:30:06.734809Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-18T13:30:05.844369Z","time spent":"890.309018ms","remote":"127.0.0.1:48126","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:11530 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-04-18T13:30:06.73728Z","caller":"traceutil/trace.go:171","msg":"trace[1707297582] linearizableReadLoop","detail":"{readStateIndex:14260; appliedIndex:14257; }","duration":"831.030548ms","start":"2025-04-18T13:30:05.906219Z","end":"2025-04-18T13:30:06.73725Z","steps":["trace[1707297582] 'read index received'  (duration: 827.996925ms)","trace[1707297582] 'applied index is now lower than readState.Index'  (duration: 3.032052ms)"],"step_count":2}
{"level":"info","ts":"2025-04-18T13:30:06.737376Z","caller":"traceutil/trace.go:171","msg":"trace[1016306596] transaction","detail":"{read_only:false; response_revision:11532; number_of_response:1; }","duration":"665.123651ms","start":"2025-04-18T13:30:06.072229Z","end":"2025-04-18T13:30:06.737353Z","steps":["trace[1016306596] 'process raft request'  (duration: 664.806227ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:30:06.737491Z","caller":"traceutil/trace.go:171","msg":"trace[1169965803] transaction","detail":"{read_only:false; response_revision:11533; number_of_response:1; }","duration":"330.60049ms","start":"2025-04-18T13:30:06.406873Z","end":"2025-04-18T13:30:06.737474Z","steps":["trace[1169965803] 'process raft request'  (duration: 330.343762ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:30:06.737515Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-18T13:30:06.07217Z","time spent":"665.256853ms","remote":"127.0.0.1:48232","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":520,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:11525 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-04-18T13:30:06.737571Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-18T13:30:06.406855Z","time spent":"330.669591ms","remote":"127.0.0.1:48232","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:11526 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2025-04-18T13:30:06.73772Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"831.536841ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-04-18T13:30:06.737789Z","caller":"traceutil/trace.go:171","msg":"trace[1392240111] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:11533; }","duration":"831.605607ms","start":"2025-04-18T13:30:05.906172Z","end":"2025-04-18T13:30:06.737778Z","steps":["trace[1392240111] 'agreement among raft nodes before linearized reading'  (duration: 831.221576ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-18T13:30:06.737821Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-18T13:30:05.906158Z","time spent":"831.655005ms","remote":"127.0.0.1:47980","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":155,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"info","ts":"2025-04-18T13:30:57.708877Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11321}
{"level":"info","ts":"2025-04-18T13:30:57.711372Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":11321,"took":"2.115947ms","hash":1404297687}
{"level":"info","ts":"2025-04-18T13:30:57.711459Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1404297687,"revision":11321,"compact-revision":10683}
{"level":"info","ts":"2025-04-18T13:31:04.690189Z","caller":"traceutil/trace.go:171","msg":"trace[222464075] transaction","detail":"{read_only:false; response_revision:11579; number_of_response:1; }","duration":"185.126932ms","start":"2025-04-18T13:31:04.505021Z","end":"2025-04-18T13:31:04.690148Z","steps":["trace[222464075] 'process raft request'  (duration: 184.925779ms)"],"step_count":1}
{"level":"info","ts":"2025-04-18T13:35:57.718611Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11573}
{"level":"info","ts":"2025-04-18T13:35:57.737435Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":11573,"took":"17.11935ms","hash":3454870156}
{"level":"info","ts":"2025-04-18T13:35:57.737569Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3454870156,"revision":11573,"compact-revision":11321}

* 
* ==> kernel <==
*  13:36:02 up  1:59,  0 users,  load average: 0.42, 0.72, 0.85
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [62da2bb5fb6f] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0418 13:14:21.122892       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0418 13:14:21.157220       1 logging.go:59] [core] [Channel #9 SubChannel #10] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0418 13:14:21.479043       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0418 13:14:21.496511       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0418 13:14:21.506349       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0418 13:14:21.521872       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0418 13:14:21.533584       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [b3c7ab0f119b] <==
* I0418 13:15:27.432840       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
E0418 13:15:38.324562       1 storage.go:475] Address {10.244.0.17  0xc005585b30 0xc000adb810} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.25}] vs 10.244.0.17 (kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-p7rrs))
E0418 13:15:38.324659       1 storage.go:485] Failed to find a valid address, skipping subset: &{[{10.244.0.17  0xc005585b30 0xc000adb810}] [] [{ 8000 TCP <nil>}]}
I0418 13:15:40.633758       1 controller.go:624] quota admission added evaluator for: endpoints
I0418 13:15:40.725499       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0418 13:17:38.831162       1 trace.go:236] Trace[585773075]: "Get" accept:application/json, */*,audit-id:d78ca928-15a8-44a2-b6f5-0b549c38daef,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (18-Apr-2025 13:17:38.214) (total time: 609ms):
Trace[585773075]: ---"About to write a response" 609ms (13:17:38.823)
Trace[585773075]: [609.961939ms] [609.961939ms] END
I0418 13:17:39.721647       1 trace.go:236] Trace[954050071]: "Update" accept:application/json, */*,audit-id:ad5edf95-0422-4283-a9e6-e3397a6d7d23,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-Apr-2025 13:17:38.925) (total time: 789ms):
Trace[954050071]: ["GuaranteedUpdate etcd3" audit-id:ad5edf95-0422-4283-a9e6-e3397a6d7d23,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 789ms (13:17:38.926)
Trace[954050071]:  ---"About to Encode" 187ms (13:17:39.113)
Trace[954050071]:  ---"Txn call completed" 517ms (13:17:39.631)
Trace[954050071]:  ---"decode succeeded" len:1020 83ms (13:17:39.715)]
Trace[954050071]: [789.532253ms] [789.532253ms] END
I0418 13:19:25.807469       1 trace.go:236] Trace[947937412]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f2a0a7bb-f9ad-4e75-8d0c-585babe01716,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (18-Apr-2025 13:19:25.110) (total time: 696ms):
Trace[947937412]: ["GuaranteedUpdate etcd3" audit-id:f2a0a7bb-f9ad-4e75-8d0c-585babe01716,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 696ms (13:19:25.110)
Trace[947937412]:  ---"About to Encode" 94ms (13:19:25.205)
Trace[947937412]:  ---"Txn call completed" 601ms (13:19:25.806)]
Trace[947937412]: [696.877782ms] [696.877782ms] END
I0418 13:19:25.808519       1 trace.go:236] Trace[173510998]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:01c4bcce-7f6a-4e89-b0cf-2e007d827d25,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (18-Apr-2025 13:19:25.202) (total time: 605ms):
Trace[173510998]: ["GuaranteedUpdate etcd3" audit-id:01c4bcce-7f6a-4e89-b0cf-2e007d827d25,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 605ms (13:19:25.203)
Trace[173510998]:  ---"Txn call completed" 601ms (13:19:25.807)]
Trace[173510998]: [605.436837ms] [605.436837ms] END
I0418 13:21:34.102213       1 trace.go:236] Trace[1919790310]: "Update" accept:application/json, */*,audit-id:69390074-9a10-4563-a73c-d15ca53ec754,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-Apr-2025 13:21:33.299) (total time: 802ms):
Trace[1919790310]: ["GuaranteedUpdate etcd3" audit-id:69390074-9a10-4563-a73c-d15ca53ec754,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 801ms (13:21:33.300)
Trace[1919790310]:  ---"Txn call completed" 797ms (13:21:34.101)]
Trace[1919790310]: [802.460355ms] [802.460355ms] END
I0418 13:21:34.498336       1 trace.go:236] Trace[1272421995]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (18-Apr-2025 13:21:33.888) (total time: 609ms):
Trace[1272421995]: ---"initial value restored" 106ms (13:21:33.995)
Trace[1272421995]: ---"Transaction prepared" 203ms (13:21:34.198)
Trace[1272421995]: ---"Txn call completed" 299ms (13:21:34.498)
Trace[1272421995]: [609.390619ms] [609.390619ms] END
I0418 13:21:35.090774       1 trace.go:236] Trace[1402229576]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:38ed94f1-0258-46f2-ac2a-2a3cf619a354,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:GET (18-Apr-2025 13:21:34.516) (total time: 574ms):
Trace[1402229576]: ---"About to write a response" 574ms (13:21:35.090)
Trace[1402229576]: [574.604449ms] [574.604449ms] END
I0418 13:21:39.891082       1 trace.go:236] Trace[1173562218]: "Update" accept:application/json, */*,audit-id:29f14855-ef4e-4c84-9218-96fa61e7117c,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-Apr-2025 13:21:39.387) (total time: 503ms):
Trace[1173562218]: ["GuaranteedUpdate etcd3" audit-id:29f14855-ef4e-4c84-9218-96fa61e7117c,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 502ms (13:21:39.388)
Trace[1173562218]:  ---"Txn call completed" 493ms (13:21:39.887)]
Trace[1173562218]: [503.263989ms] [503.263989ms] END
I0418 13:21:50.187933       1 trace.go:236] Trace[975795166]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f647d87c-fe09-47ee-bf65-2cafbd943a51,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (18-Apr-2025 13:21:49.316) (total time: 871ms):
Trace[975795166]: ["GuaranteedUpdate etcd3" audit-id:f647d87c-fe09-47ee-bf65-2cafbd943a51,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 871ms (13:21:49.316)
Trace[975795166]:  ---"initial value restored" 370ms (13:21:49.687)
Trace[975795166]:  ---"Txn call completed" 499ms (13:21:50.187)]
Trace[975795166]: [871.547892ms] [871.547892ms] END
I0418 13:21:50.687524       1 trace.go:236] Trace[179743595]: "Update" accept:application/json, */*,audit-id:333134d5-bc7e-452d-8e5b-7aa881d7a534,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-Apr-2025 13:21:49.987) (total time: 699ms):
Trace[179743595]: ---"limitedReadBody succeeded" len:1354 99ms (13:21:50.087)
Trace[179743595]: ["GuaranteedUpdate etcd3" audit-id:333134d5-bc7e-452d-8e5b-7aa881d7a534,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 600ms (13:21:50.087)
Trace[179743595]:  ---"Txn call completed" 590ms (13:21:50.687)]
Trace[179743595]: [699.96573ms] [699.96573ms] END
I0418 13:30:06.735809       1 trace.go:236] Trace[676424676]: "Update" accept:application/json, */*,audit-id:0955b3da-2272-4780-b5fa-6f6256e0a84c,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-Apr-2025 13:30:05.842) (total time: 893ms):
Trace[676424676]: ["GuaranteedUpdate etcd3" audit-id:0955b3da-2272-4780-b5fa-6f6256e0a84c,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 893ms (13:30:05.842)
Trace[676424676]:  ---"Txn call completed" 892ms (13:30:06.735)]
Trace[676424676]: [893.71588ms] [893.71588ms] END
I0418 13:30:06.738518       1 trace.go:236] Trace[104080393]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f9a5aa7a-df68-494a-8d11-af826a35ec6a,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (18-Apr-2025 13:30:06.070) (total time: 667ms):
Trace[104080393]: ["GuaranteedUpdate etcd3" audit-id:f9a5aa7a-df68-494a-8d11-af826a35ec6a,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 667ms (13:30:06.071)
Trace[104080393]:  ---"Txn call completed" 666ms (13:30:06.738)]
Trace[104080393]: [667.502904ms] [667.502904ms] END
I0418 13:30:06.747913       1 trace.go:236] Trace[1036309536]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (18-Apr-2025 13:30:05.905) (total time: 842ms):
Trace[1036309536]: ---"initial value restored" 833ms (13:30:06.738)
Trace[1036309536]: [842.57104ms] [842.57104ms] END

* 
* ==> kube-controller-manager [327cc35e80df] <==
* I0418 12:33:45.962394       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="csr-9fhx6" approvedExpiration="1h0m0s"
I0418 12:42:04.041290       1 event.go:307] "Event occurred" object="default/catalog-service" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set catalog-service-6f5d4b4f76 to 1"
I0418 12:42:04.060593       1 event.go:307] "Event occurred" object="default/catalog-service-6f5d4b4f76" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: catalog-service-6f5d4b4f76-966d9"
I0418 12:42:04.070845       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="31.241652ms"
I0418 12:42:04.086478       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="15.516335ms"
I0418 12:42:04.087603       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="205.551s"
I0418 12:42:04.167002       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="1.370547ms"
I0418 12:42:08.451929       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="986.523s"
I0418 12:42:21.528216       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="2.664407ms"
I0418 12:42:35.519184       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="313.066s"
I0418 12:42:47.505981       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="206.862s"
I0418 12:43:02.512911       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="144.096s"
I0418 12:43:13.502680       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="173.563s"
I0418 12:43:49.508956       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="348.382s"
I0418 12:44:00.506223       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="591.346s"
I0418 12:45:19.519895       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="660.055s"
I0418 12:45:32.497963       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="100.985s"
I0418 12:48:03.498115       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="86.163s"
I0418 12:48:16.502992       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="130.181s"
I0418 12:53:11.510568       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="157.817s"
I0418 12:53:26.958695       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="215.037s"
I0418 12:53:28.877797       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="95.35199ms"
I0418 12:53:28.879119       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="228.78s"
I0418 12:54:31.797265       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="24.680447ms"
I0418 12:54:31.797514       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="104.185s"
I0418 12:54:33.076738       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="74.543343ms"
I0418 12:54:33.077125       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="180.232s"
I0418 12:55:29.069430       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="14.050999ms"
I0418 12:55:29.069757       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="172.367s"
I0418 12:55:40.495533       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="970.185s"
I0418 12:55:41.570256       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="101.254763ms"
I0418 12:55:41.573374       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="138.019s"
I0418 12:56:37.398663       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="21.927783ms"
I0418 12:56:37.398967       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="113.708s"
I0418 12:56:50.489249       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="94.761s"
I0418 12:57:02.903697       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="16.622902ms"
I0418 12:57:02.903847       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="60.429s"
I0418 12:58:01.494532       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="22.8714ms"
I0418 12:58:01.494848       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="141.161s"
I0418 12:58:13.488595       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="137.012s"
I0418 12:58:47.571652       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="83.019619ms"
I0418 12:58:47.571957       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="76.384s"
I0418 12:59:38.604534       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="24.696828ms"
I0418 12:59:38.604746       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="96.862s"
I0418 12:59:52.499100       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="311.803s"
I0418 13:01:12.247080       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="290.364012ms"
I0418 13:01:12.247497       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="278.924s"
I0418 13:02:01.471563       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="22.481111ms"
I0418 13:02:01.471882       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="128.607s"
I0418 13:02:13.484149       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="78.132s"
I0418 13:04:51.146298       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="102.006857ms"
I0418 13:04:51.146535       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="94.988s"
I0418 13:05:49.258014       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="20.462843ms"
I0418 13:05:49.258255       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="79.06s"
I0418 13:06:04.476179       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="143.286s"
I0418 13:10:58.955762       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="22.285419ms"
I0418 13:10:58.956232       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="120.788s"
I0418 13:11:51.145675       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="29.866935ms"
I0418 13:11:51.162720       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="138.668s"
I0418 13:12:05.463698       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="70.752s"

* 
* ==> kube-controller-manager [76b40d46196e] <==
* I0418 13:15:40.127689       1 shared_informer.go:318] Caches are synced for ephemeral
I0418 13:15:40.128975       1 shared_informer.go:318] Caches are synced for daemon sets
I0418 13:15:40.130256       1 shared_informer.go:318] Caches are synced for endpoint
I0418 13:15:40.133974       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0418 13:15:40.130346       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0418 13:15:40.130393       1 shared_informer.go:318] Caches are synced for HPA
I0418 13:15:40.130453       1 shared_informer.go:318] Caches are synced for attach detach
I0418 13:15:40.133285       1 shared_informer.go:318] Caches are synced for deployment
I0418 13:15:40.230169       1 shared_informer.go:318] Caches are synced for job
I0418 13:15:40.230836       1 shared_informer.go:318] Caches are synced for taint
I0418 13:15:40.231526       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0418 13:15:40.231868       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0418 13:15:40.232077       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0418 13:15:40.232164       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0418 13:15:40.232301       1 taint_manager.go:211] "Sending events to api server"
I0418 13:15:40.235436       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0418 13:15:40.235690       1 shared_informer.go:318] Caches are synced for disruption
I0418 13:15:40.236346       1 shared_informer.go:318] Caches are synced for resource quota
I0418 13:15:40.236394       1 shared_informer.go:318] Caches are synced for resource quota
I0418 13:15:40.519933       1 shared_informer.go:318] Caches are synced for garbage collector
I0418 13:15:40.519977       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0418 13:15:40.520928       1 shared_informer.go:318] Caches are synced for garbage collector
I0418 13:15:40.824604       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="701.776954ms"
I0418 13:15:40.824810       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="69.05s"
I0418 13:15:41.124611       1 endpointslice_controller.go:310] "Error syncing endpoint slices for service, retrying" key="kube-system/kube-dns" err="EndpointSlice informer cache is out of date"
I0418 13:15:41.124866       1 endpointslice_controller.go:310] "Error syncing endpoint slices for service, retrying" key="default/polar-postgres" err="EndpointSlice informer cache is out of date"
I0418 13:15:41.124996       1 endpointslice_controller.go:310] "Error syncing endpoint slices for service, retrying" key="kubernetes-dashboard/kubernetes-dashboard" err="EndpointSlice informer cache is out of date"
I0418 13:15:41.125000       1 endpointslice_controller.go:310] "Error syncing endpoint slices for service, retrying" key="kubernetes-dashboard/dashboard-metrics-scraper" err="EndpointSlice informer cache is out of date"
I0418 13:15:41.129734       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="1.006613069s"
I0418 13:15:41.218877       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="162.939s"
I0418 13:15:50.120396       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="194.434174ms"
I0418 13:15:50.120785       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="148.64s"
I0418 13:16:38.166270       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="10.475234ms"
I0418 13:16:38.166570       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="82.784s"
I0418 13:16:53.363270       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="57.727s"
I0418 13:16:54.922390       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="81.309016ms"
I0418 13:16:54.922670       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="82.168s"
I0418 13:17:47.123805       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="66.638652ms"
I0418 13:17:47.124793       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="167.469s"
I0418 13:17:58.363380       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="83.123s"
I0418 13:19:05.526138       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="21.683263ms"
I0418 13:19:05.526749       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="165.269s"
I0418 13:20:05.508036       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="6.734408ms"
I0418 13:20:05.508157       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="51.315s"
I0418 13:20:19.552053       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="99.262s"
I0418 13:20:50.323384       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="15.649594ms"
I0418 13:20:50.323943       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="117.624s"
I0418 13:21:54.608585       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="64.382394ms"
I0418 13:21:54.609103       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="378.448s"
I0418 13:22:06.542669       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="71.493s"
I0418 13:27:39.753164       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="17.576941ms"
I0418 13:27:39.753335       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="83.167s"
I0418 13:28:21.183683       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="8.424984ms"
I0418 13:28:21.183778       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="29.907s"
I0418 13:28:33.438115       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="74.82s"
I0418 13:31:04.891080       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="199.027294ms"
I0418 13:31:04.891212       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="68.928s"
I0418 13:31:37.896527       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="12.129522ms"
I0418 13:31:37.896661       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="54.125s"
I0418 13:31:51.434238       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/catalog-service-6f5d4b4f76" duration="97.763s"

* 
* ==> kube-proxy [7ffcec4c346e] <==
* I0418 11:32:37.899464       1 server_others.go:69] "Using iptables proxy"
I0418 11:32:37.912353       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0418 11:32:37.944695       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0418 11:32:37.947155       1 server_others.go:152] "Using iptables Proxier"
I0418 11:32:37.947224       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0418 11:32:37.947233       1 server_others.go:438] "Defaulting to no-op detect-local"
I0418 11:32:37.947334       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0418 11:32:37.947818       1 server.go:846] "Version info" version="v1.28.3"
I0418 11:32:37.947866       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0418 11:32:37.949210       1 config.go:97] "Starting endpoint slice config controller"
I0418 11:32:37.949339       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0418 11:32:37.949226       1 config.go:188] "Starting service config controller"
I0418 11:32:37.949399       1 shared_informer.go:311] Waiting for caches to sync for service config
I0418 11:32:37.949253       1 config.go:315] "Starting node config controller"
I0418 11:32:37.949408       1 shared_informer.go:311] Waiting for caches to sync for node config
I0418 11:32:38.050488       1 shared_informer.go:318] Caches are synced for service config
I0418 11:32:38.050527       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0418 11:32:38.050502       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [81ea107af513] <==
* I0418 13:15:29.331270       1 server_others.go:69] "Using iptables proxy"
I0418 13:15:29.928793       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0418 13:15:31.527498       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0418 13:15:31.623870       1 server_others.go:152] "Using iptables Proxier"
I0418 13:15:31.624222       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0418 13:15:31.624419       1 server_others.go:438] "Defaulting to no-op detect-local"
I0418 13:15:31.625436       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0418 13:15:31.630337       1 server.go:846] "Version info" version="v1.28.3"
I0418 13:15:31.630527       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0418 13:15:31.832636       1 config.go:188] "Starting service config controller"
I0418 13:15:31.833730       1 shared_informer.go:311] Waiting for caches to sync for service config
I0418 13:15:31.834314       1 config.go:97] "Starting endpoint slice config controller"
I0418 13:15:31.834350       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0418 13:15:31.919885       1 config.go:315] "Starting node config controller"
I0418 13:15:31.919937       1 shared_informer.go:311] Waiting for caches to sync for node config
I0418 13:15:32.119493       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0418 13:15:32.119679       1 shared_informer.go:318] Caches are synced for service config
I0418 13:15:32.120872       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [522f214ebaae] <==
* I0418 13:15:13.130018       1 serving.go:348] Generated self-signed cert in-memory
W0418 13:15:18.424968       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0418 13:15:18.425087       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0418 13:15:18.425128       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0418 13:15:18.425150       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0418 13:15:18.822661       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0418 13:15:18.822754       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0418 13:15:18.830214       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0418 13:15:18.830755       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0418 13:15:18.831897       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0418 13:15:18.832042       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0418 13:15:18.931396       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [f4aad0be47e9] <==
* I0418 11:32:33.322064       1 serving.go:348] Generated self-signed cert in-memory
W0418 11:32:35.900152       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0418 11:32:35.900189       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0418 11:32:35.900202       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0418 11:32:35.900210       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0418 11:32:36.006425       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0418 11:32:36.006665       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0418 11:32:36.008943       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0418 11:32:36.009101       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0418 11:32:36.009875       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0418 11:32:36.010322       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0418 11:32:36.110517       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0418 13:14:11.124567       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0418 13:14:11.125508       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0418 13:14:11.223470       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Apr 18 13:29:29 minikube kubelet[1663]: I0418 13:29:29.422977    1663 scope.go:117] "RemoveContainer" containerID="f1b9e6fc055bd84510d1615352f3648738e99a8708d6952858df52289f477538"
Apr 18 13:29:29 minikube kubelet[1663]: E0418 13:29:29.423591    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:29:40 minikube kubelet[1663]: I0418 13:29:40.422608    1663 scope.go:117] "RemoveContainer" containerID="f1b9e6fc055bd84510d1615352f3648738e99a8708d6952858df52289f477538"
Apr 18 13:29:40 minikube kubelet[1663]: E0418 13:29:40.422934    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:29:55 minikube kubelet[1663]: I0418 13:29:55.423530    1663 scope.go:117] "RemoveContainer" containerID="f1b9e6fc055bd84510d1615352f3648738e99a8708d6952858df52289f477538"
Apr 18 13:29:55 minikube kubelet[1663]: E0418 13:29:55.424343    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:30:10 minikube kubelet[1663]: I0418 13:30:10.421499    1663 scope.go:117] "RemoveContainer" containerID="f1b9e6fc055bd84510d1615352f3648738e99a8708d6952858df52289f477538"
Apr 18 13:30:10 minikube kubelet[1663]: E0418 13:30:10.422043    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:30:25 minikube kubelet[1663]: I0418 13:30:25.421393    1663 scope.go:117] "RemoveContainer" containerID="f1b9e6fc055bd84510d1615352f3648738e99a8708d6952858df52289f477538"
Apr 18 13:30:25 minikube kubelet[1663]: E0418 13:30:25.421712    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:30:40 minikube kubelet[1663]: I0418 13:30:40.421208    1663 scope.go:117] "RemoveContainer" containerID="f1b9e6fc055bd84510d1615352f3648738e99a8708d6952858df52289f477538"
Apr 18 13:30:40 minikube kubelet[1663]: E0418 13:30:40.421611    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:30:49 minikube kubelet[1663]: W0418 13:30:49.595812    1663 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 18 13:30:52 minikube kubelet[1663]: I0418 13:30:52.420976    1663 scope.go:117] "RemoveContainer" containerID="f1b9e6fc055bd84510d1615352f3648738e99a8708d6952858df52289f477538"
Apr 18 13:30:52 minikube kubelet[1663]: E0418 13:30:52.421450    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:31:03 minikube kubelet[1663]: I0418 13:31:03.421362    1663 scope.go:117] "RemoveContainer" containerID="f1b9e6fc055bd84510d1615352f3648738e99a8708d6952858df52289f477538"
Apr 18 13:31:37 minikube kubelet[1663]: I0418 13:31:37.865101    1663 scope.go:117] "RemoveContainer" containerID="f1b9e6fc055bd84510d1615352f3648738e99a8708d6952858df52289f477538"
Apr 18 13:31:37 minikube kubelet[1663]: I0418 13:31:37.865481    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:31:37 minikube kubelet[1663]: E0418 13:31:37.865809    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:31:51 minikube kubelet[1663]: I0418 13:31:51.419860    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:31:51 minikube kubelet[1663]: E0418 13:31:51.420516    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:32:03 minikube kubelet[1663]: I0418 13:32:03.419395    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:32:03 minikube kubelet[1663]: E0418 13:32:03.419823    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:32:17 minikube kubelet[1663]: I0418 13:32:17.419257    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:32:17 minikube kubelet[1663]: E0418 13:32:17.419790    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:32:32 minikube kubelet[1663]: I0418 13:32:32.417559    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:32:32 minikube kubelet[1663]: E0418 13:32:32.417918    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:32:46 minikube kubelet[1663]: I0418 13:32:46.417631    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:32:46 minikube kubelet[1663]: E0418 13:32:46.417983    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:33:00 minikube kubelet[1663]: I0418 13:33:00.418231    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:33:00 minikube kubelet[1663]: E0418 13:33:00.418735    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:33:15 minikube kubelet[1663]: I0418 13:33:15.417906    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:33:15 minikube kubelet[1663]: E0418 13:33:15.418380    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:33:28 minikube kubelet[1663]: I0418 13:33:28.419108    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:33:28 minikube kubelet[1663]: E0418 13:33:28.420233    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:33:43 minikube kubelet[1663]: I0418 13:33:43.417640    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:33:43 minikube kubelet[1663]: E0418 13:33:43.417922    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:33:55 minikube kubelet[1663]: I0418 13:33:55.417259    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:33:55 minikube kubelet[1663]: E0418 13:33:55.417702    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:34:09 minikube kubelet[1663]: I0418 13:34:09.418407    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:34:09 minikube kubelet[1663]: E0418 13:34:09.418790    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:34:22 minikube kubelet[1663]: I0418 13:34:22.418025    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:34:22 minikube kubelet[1663]: E0418 13:34:22.419170    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:34:37 minikube kubelet[1663]: I0418 13:34:37.418239    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:34:37 minikube kubelet[1663]: E0418 13:34:37.418597    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:34:48 minikube kubelet[1663]: I0418 13:34:48.417775    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:34:48 minikube kubelet[1663]: E0418 13:34:48.418359    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:35:01 minikube kubelet[1663]: I0418 13:35:01.418409    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:35:01 minikube kubelet[1663]: E0418 13:35:01.418973    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:35:13 minikube kubelet[1663]: I0418 13:35:13.418632    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:35:13 minikube kubelet[1663]: E0418 13:35:13.418918    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:35:25 minikube kubelet[1663]: I0418 13:35:25.417681    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:35:25 minikube kubelet[1663]: E0418 13:35:25.418009    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:35:36 minikube kubelet[1663]: I0418 13:35:36.417852    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:35:36 minikube kubelet[1663]: E0418 13:35:36.418310    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:35:49 minikube kubelet[1663]: I0418 13:35:49.418471    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:35:49 minikube kubelet[1663]: E0418 13:35:49.418755    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"
Apr 18 13:35:49 minikube kubelet[1663]: W0418 13:35:49.593143    1663 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 18 13:36:00 minikube kubelet[1663]: I0418 13:36:00.418117    1663 scope.go:117] "RemoveContainer" containerID="49853b7570c73a3632dae182c01da372b7e50ad8c21e89cfd1fa5885a72bad67"
Apr 18 13:36:00 minikube kubelet[1663]: E0418 13:36:00.418446    1663 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"catalog-service\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=catalog-service pod=catalog-service-6f5d4b4f76-966d9_default(4fe28161-49d6-4fd8-9d96-9f5993bb0a7c)\"" pod="default/catalog-service-6f5d4b4f76-966d9" podUID="4fe28161-49d6-4fd8-9d96-9f5993bb0a7c"

* 
* ==> kubernetes-dashboard [353c7589ee66] <==
* 2025/04/18 13:15:35 Starting overwatch
2025/04/18 13:15:35 Using namespace: kubernetes-dashboard
2025/04/18 13:15:35 Using in-cluster config to connect to apiserver
2025/04/18 13:15:35 Using secret token for csrf signing
2025/04/18 13:15:35 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/04/18 13:15:36 Successful initial request to the apiserver, version: v1.28.3
2025/04/18 13:15:36 Generating JWE encryption key
2025/04/18 13:15:36 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2025/04/18 13:15:37 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2025/04/18 13:15:38 Initializing JWE encryption key from synchronized object
2025/04/18 13:15:38 Creating in-cluster Sidecar client
2025/04/18 13:15:38 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2025/04/18 13:15:38 Serving insecurely on HTTP port: 9090
2025/04/18 13:16:08 Successful request to sidecar

* 
* ==> kubernetes-dashboard [b2f3ba3aa82d] <==
* 2025/04/18 13:09:19 received 0 resources from sidecar instead of 2
2025/04/18 13:09:19 [2025-04-18T13:09:19Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/18 13:09:19 Getting list of all replication controllers in the cluster
2025/04/18 13:09:19 received 0 resources from sidecar instead of 2
2025/04/18 13:09:19 received 0 resources from sidecar instead of 2
2025/04/18 13:09:19 [2025-04-18T13:09:19Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:19 received 0 resources from sidecar instead of 2
2025/04/18 13:09:19 [2025-04-18T13:09:19Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/18 13:09:19 Getting list of all pet sets in the cluster
2025/04/18 13:09:19 Getting pod metrics
2025/04/18 13:09:19 [2025-04-18T13:09:19Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:19 received 0 resources from sidecar instead of 2
2025/04/18 13:09:19 [2025-04-18T13:09:19Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:19 [2025-04-18T13:09:19Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:19 received 0 resources from sidecar instead of 2
2025/04/18 13:09:19 received 0 resources from sidecar instead of 2
2025/04/18 13:09:19 Skipping metric because of error: Metric label not set.
2025/04/18 13:09:19 Skipping metric because of error: Metric label not set.
2025/04/18 13:09:19 Skipping metric because of error: Metric label not set.
2025/04/18 13:09:19 Skipping metric because of error: Metric label not set.
2025/04/18 13:09:19 [2025-04-18T13:09:19Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/04/18 13:09:21 Getting list of namespaces
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/18 13:09:21 Getting list of all cron jobs in the cluster
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/18 13:09:21 Getting list of all jobs in the cluster
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/18 13:09:21 Getting list of all deployments in the cluster
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/18 13:09:21 Getting list of all pods in the cluster
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/18 13:09:21 Getting list of all replica sets in the cluster
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/18 13:09:21 Getting list of all replication controllers in the cluster
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/18 13:09:21 Getting list of all pet sets in the cluster
2025/04/18 13:09:21 received 0 resources from sidecar instead of 2
2025/04/18 13:09:21 received 0 resources from sidecar instead of 2
2025/04/18 13:09:21 received 0 resources from sidecar instead of 2
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:21 received 0 resources from sidecar instead of 2
2025/04/18 13:09:21 received 0 resources from sidecar instead of 2
2025/04/18 13:09:21 Getting pod metrics
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:21 received 0 resources from sidecar instead of 2
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/18 13:09:21 received 0 resources from sidecar instead of 2
2025/04/18 13:09:21 received 0 resources from sidecar instead of 2
2025/04/18 13:09:21 Skipping metric because of error: Metric label not set.
2025/04/18 13:09:21 Skipping metric because of error: Metric label not set.
2025/04/18 13:09:21 Skipping metric because of error: Metric label not set.
2025/04/18 13:09:21 Skipping metric because of error: Metric label not set.
2025/04/18 13:09:21 [2025-04-18T13:09:21Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [56033ca65028] <==
* I0418 13:15:26.940349       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0418 13:15:37.441908       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

* 
* ==> storage-provisioner [a4fa4909329e] <==
* I0418 13:15:56.316832       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0418 13:15:56.356564       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0418 13:15:56.358168       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0418 13:16:13.865059       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0418 13:16:13.866540       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_b4bd0462-5a83-4d10-9e6f-c78e85903301!
I0418 13:16:13.867912       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"f57d8806-0277-4443-a385-13aba43b466b", APIVersion:"v1", ResourceVersion:"11101", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_b4bd0462-5a83-4d10-9e6f-c78e85903301 became leader
I0418 13:16:13.967964       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_b4bd0462-5a83-4d10-9e6f-c78e85903301!

